# -*- coding: utf-8 -*-
"""
è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿ - ä¸»çˆ¬è™«è„šæœ¬
æ”¯æŒæŒ‰ç±»å‹åˆ†ç±»ä¿å­˜ã€æ–­ç‚¹ç»­çˆ¬ã€å¤šè´¦å·ç®¡ç†ã€ä»»åŠ¡è°ƒåº¦
åˆ›å»ºæ—¶é—´: 2025-01-23
"""

import os
import json
import time
import csv
import argparse
import logging
import random
from datetime import date, datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import requests
from dotenv import load_dotenv
import pymysql
from pymysql.cursors import DictCursor

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class DarenCrawler:
    def __init__(self):
        self.base_url = "https://agent.oceanengine.com/star/mirror/gw/api/gsearch/search_for_author_square"
        self.setup_logging()
        self.setup_directories()
        self.db_config = self.get_db_config()
        self.current_account = None
        self.task_id = None
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        log_filename = f"crawler_{date.today().strftime('%Y%m%d')}.log"
        log_path = log_dir / log_filename
        
        logging.basicConfig(
            level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
            format=os.getenv('LOG_FORMAT', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'),
            handlers=[
                logging.FileHandler(log_path, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        directories = [
            "logs",
            "data_by_type", 
            "exports",
            "temp",
            "backups"
        ]
        for dir_name in directories:
            Path(dir_name).mkdir(exist_ok=True)
            
    def get_db_config(self):
        """è·å–æ•°æ®åº“é…ç½®"""
        return {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 3306)),
            'user': os.getenv('DB_USER', 'root'),
            'password': os.getenv('DB_PASSWORD', ''),
            'database': os.getenv('DB_NAME', 'influencer_crawler'),
            'charset': 'utf8mb4'
        }
        
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            return pymysql.connect(**self.db_config, cursorclass=DictCursor)
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return None
            
    def load_account_config(self, account_id: int = None):
        """åŠ è½½è´¦å·é…ç½®"""
        conn = self.get_db_connection()
        if not conn:
            return None
            
        try:
            with conn.cursor() as cursor:
                if account_id:
                    cursor.execute(
                        "SELECT * FROM daoren_account WHERE id = %s AND status = 'active'",
                        (account_id,)
                    )
                else:
                    # é€‰æ‹©æœ€å°‘ä½¿ç”¨çš„æ´»è·ƒè´¦å·
                    cursor.execute("""
                        SELECT * FROM daoren_account 
                        WHERE status = 'active' 
                        AND (cooldown_until IS NULL OR cooldown_until < NOW())
                        ORDER BY last_used_at ASC, success_count ASC 
                        LIMIT 1
                    """)
                
                account = cursor.fetchone()
                if account:
                    self.current_account = account
                    # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
                    cursor.execute(
                        "UPDATE daoren_account SET last_used_at = NOW() WHERE id = %s",
                        (account['id'],)
                    )
                    conn.commit()
                    self.logger.info(f"ä½¿ç”¨è´¦å·: {account['account_name']}")
                    return account
                else:
                    self.logger.warning("æ²¡æœ‰å¯ç”¨çš„æ´»è·ƒè´¦å·")
                    return None
                    
        except Exception as e:
            self.logger.error(f"åŠ è½½è´¦å·é…ç½®å¤±è´¥: {e}")
            return None
        finally:
            conn.close()
            
    def get_headers(self):
        """è·å–è¯·æ±‚å¤´"""
        if self.current_account and self.current_account.get('headers'):
            try:
                custom_headers = json.loads(self.current_account['headers'])
                return custom_headers
            except:
                pass
                
        # é»˜è®¤è¯·æ±‚å¤´
        return {
            'accept': 'application/json, text/plain, */*',
            'accept-language': 'zh-CN,zh;q=0.9',
            'agw-js-conv': 'str',
            'content-type': 'application/json',
            'origin': 'https://agent.oceanengine.com',
            'priority': 'u=1, i',
            'referer': 'https://agent.oceanengine.com/admin/star-agent/vue2/market',
            'sec-ch-ua': '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',
            'x-login-source': '1',
            'x-tt-possess-scene': '2',
            'x-tt-possess-star-id': '1843934177451019',
            'cookie': self.current_account.get('cookies', '') if self.current_account else ''
        }
        
    def create_task(self, task_name: str, start_page: int, end_page: int, domain_filter: str = None):
        """åˆ›å»ºçˆ¬å–ä»»åŠ¡"""
        conn = self.get_db_connection()
        if not conn:
            return None
            
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO daoren_task_log 
                    (task_name, domain_filter, start_page, end_page, status, start_time)
                    VALUES (%s, %s, %s, %s, 'running', NOW())
                """, (task_name, domain_filter, start_page, end_page))
                
                task_id = cursor.lastrowid
                conn.commit()
                self.task_id = task_id
                self.logger.info(f"åˆ›å»ºä»»åŠ¡ ID: {task_id}, åç§°: {task_name}")
                return task_id
                
        except Exception as e:
            self.logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
            return None
        finally:
            conn.close()
            
    def update_task_progress(self, current_page: int, total_authors: int = None, 
                           success_authors: int = None, failed_authors: int = None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        if not self.task_id:
            return
            
        conn = self.get_db_connection()
        if not conn:
            return
            
        try:
            with conn.cursor() as cursor:
                update_fields = ["current_page = %s"]
                params = [current_page]
                
                if total_authors is not None:
                    update_fields.append("total_authors = %s")
                    params.append(total_authors)
                    
                if success_authors is not None:
                    update_fields.append("success_authors = %s")
                    params.append(success_authors)
                    
                if failed_authors is not None:
                    update_fields.append("failed_authors = %s")
                    params.append(failed_authors)
                    
                params.append(self.task_id)
                
                cursor.execute(f"""
                    UPDATE daoren_task_log 
                    SET {', '.join(update_fields)}
                    WHERE id = %s
                """, params)
                
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")
        finally:
            conn.close()
            
    def complete_task(self, status: str = 'completed', error_message: str = None):
        """å®Œæˆä»»åŠ¡"""
        if not self.task_id:
            return
            
        conn = self.get_db_connection()
        if not conn:
            return
            
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE daoren_task_log 
                    SET status = %s, end_time = NOW(), error_message = %s
                    WHERE id = %s
                """, (status, error_message, self.task_id))
                
                conn.commit()
                self.logger.info(f"ä»»åŠ¡ {self.task_id} çŠ¶æ€æ›´æ–°ä¸º: {status}")
                
        except Exception as e:
            self.logger.error(f"å®Œæˆä»»åŠ¡å¤±è´¥: {e}")
        finally:
            conn.close()
            
    def fetch_daren_data_by_page(self, page_num: int) -> Dict[str, Any]:
        """è·å–æŒ‡å®šé¡µç çš„è¾¾äººæ•°æ®"""
        headers = self.get_headers()
        
        payload = {
            "scene_param": {
                "platform_source": 1,
                "search_scene": 1,
                "display_scene": 1,
                "marketing_target": 1,
                "task_category": 1,
                "first_industry_id": 0,
                "task_status": 3
            },
            "search_param": {
                "seach_type": 2
            },
            "sort_param": {
                "sort_type": 2,
                "sort_field": {
                    "field_name": "score"
                }
            },
            "page_param": {
                "page": page_num,
                "limit": 20
            },
            "attribute_filter": [
                {
                    "field": {
                        "field_name": "tag_level_two"
                    },
                    "field_value": "[2]"
                },
                {
                    "field": {
                        "field_name": "price_by_video_type__ge",
                        "rel_id": "2"
                    },
                    "field_value": "0"
                }
            ]
        }
        
        try:
            self.logger.info(f"æ­£åœ¨è¯·æ±‚ç¬¬ {page_num} é¡µæ•°æ®...")
            
            response = requests.post(
                self.base_url, 
                headers=headers, 
                json=payload, 
                timeout=int(os.getenv('API_TIMEOUT', 30))
            )
            response.raise_for_status()
            
            # æ›´æ–°è´¦å·æˆåŠŸè®¡æ•°
            if self.current_account:
                self.update_account_stats(self.current_account['id'], success=True)
                
            return response.json()
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è¯·æ±‚ç¬¬ {page_num} é¡µå¤±è´¥: {e}")
            
            # æ›´æ–°è´¦å·å¤±è´¥è®¡æ•°
            if self.current_account:
                self.update_account_stats(self.current_account['id'], success=False)
                
            return {}
            
    def update_account_stats(self, account_id: int, success: bool):
        """æ›´æ–°è´¦å·ç»Ÿè®¡ä¿¡æ¯"""
        conn = self.get_db_connection()
        if not conn:
            return
            
        try:
            with conn.cursor() as cursor:
                if success:
                    cursor.execute(
                        "UPDATE daoren_account SET success_count = success_count + 1 WHERE id = %s",
                        (account_id,)
                    )
                else:
                    cursor.execute(
                        "UPDATE daoren_account SET failed_count = failed_count + 1 WHERE id = %s",
                        (account_id,)
                    )
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"æ›´æ–°è´¦å·ç»Ÿè®¡å¤±è´¥: {e}")
        finally:
            conn.close()
            
    def parse_author_data(self, author: Dict[str, Any], page_num: int) -> Dict[str, Any]:
        """è§£æå•ä¸ªè¾¾äººçš„æ•°æ®"""
        attr_data = author.get('attribute_datas', {})
        
        # å®‰å…¨è·å–task_infoï¼Œé¿å…list index out of rangeé”™è¯¯
        task_infos = author.get('task_infos', [])
        task_info = task_infos[0] if task_infos else {}
        
        # å®‰å…¨è·å–price_infoï¼Œé¿å…list index out of rangeé”™è¯¯
        price_infos = task_info.get('price_infos', [])
        price_info = price_infos[0] if price_infos else {}
        
        # å®‰å…¨è·å–æ•°å€¼ï¼Œé¿å…Noneå’ŒNaN
        def safe_get_number(data, key, default=0):
            value = data.get(key, default)
            if value is None or str(value).lower() == 'nan':
                return default
            try:
                return int(value) if isinstance(value, (int, float)) and value == int(value) else float(value)
            except (ValueError, TypeError):
                return default
                
        def safe_get_string(data, key, default=''):
            value = data.get(key, default)
            if value is None or str(value).lower() == 'nan':
                return default
            return str(value).strip()
        
        # æå–æ ¸å¿ƒå­—æ®µ
        parsed_data = {
            "è¾¾äººID (star_id)": safe_get_string(author, 'star_id'),
            "æ˜µç§° (nick_name)": safe_get_string(attr_data, 'nick_name'),
            "ç²‰ä¸æ•° (follower)": safe_get_number(attr_data, 'follower', 0),
            "æ‰€åœ¨åœ° (city)": safe_get_string(attr_data, 'city'),
            "è¿‘30å¤©å¹³å‡æ’­æ”¾é‡ (vv_median_30d)": safe_get_number(attr_data, 'vv_median_30d', 0),
            "è¿‘30å¤©äº’åŠ¨ç‡ (interact_rate_within_30d)": safe_get_number(attr_data, 'interact_rate_within_30d', 0.0),
            "æŠ¥ä»· (price)": safe_get_number(price_info, 'price', 0),
            "æ˜Ÿå›¾æŒ‡æ•° (star_index)": safe_get_number(attr_data, 'star_index', 0),
            "ç”µå•†ç­‰çº§ (author_ecom_level)": safe_get_string(attr_data, 'author_ecom_level'),
            "å†…å®¹ä¸»é¢˜æ ‡ç­¾ (content_theme_labels_180d)": safe_get_string(attr_data, 'content_theme_labels_180d'),
            "é¡µç ": page_num,
            "çˆ¬å–æ—¥æœŸ": date.today().strftime('%Y-%m-%d')
        }
        
        # æå–è¾¾äººç±»å‹
        tags_relation_str = attr_data.get('tags_relation', '{}')
        try:
            # å®‰å…¨å¤„ç†tags_relationå­—æ®µ
            if tags_relation_str is None or str(tags_relation_str).lower() == 'nan':
                tags_relation_str = '{}'
            elif not isinstance(tags_relation_str, str):
                tags_relation_str = str(tags_relation_str)
                
            tags_relation_dict = json.loads(tags_relation_str)
            è¾¾äººç±»å‹åˆ—è¡¨ = [tag for tag in tags_relation_dict.keys() if tag and str(tag).strip()]
        except (json.JSONDecodeError, TypeError, AttributeError):
            è¾¾äººç±»å‹åˆ—è¡¨ = []
            
        parsed_data["è¾¾äººç±»å‹ (tags)"] = è¾¾äººç±»å‹åˆ—è¡¨
        
        return parsed_data
        
    def ensure_directory(self, path: str):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        Path(path).mkdir(parents=True, exist_ok=True)
        
    def save_author_to_type_folder(self, author_data: Dict[str, Any], page_num: int, crawl_date: str):
        """æ ¹æ®è¾¾äººç±»å‹ï¼Œä¿å­˜åˆ°å¯¹åº”æ–‡ä»¶å¤¹ä¸‹çš„ç‹¬ç«‹CSV"""
        tags = author_data.get("è¾¾äººç±»å‹ (tags)", [])
        nick_name = author_data.get("æ˜µç§° (nick_name)", "æœªçŸ¥æ˜µç§°")
        
        # ç¡®ä¿nick_nameä¸æ˜¯NaNæˆ–None
        if not nick_name or str(nick_name).lower() == 'nan':
            nick_name = "æœªçŸ¥æ˜µç§°"
        
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        safe_nick_name = "".join(c for c in str(nick_name) if c.isalnum() or c in (' ', '-', '_')).strip()
        if not safe_nick_name:
            safe_nick_name = f"è¾¾äºº_{author_data.get('è¾¾äººID (star_id)', 'unknown')}"
            
        # å¦‚æœæ²¡æœ‰ç±»å‹ï¼Œé»˜è®¤å­˜å…¥ "æœªåˆ†ç±»"
        if not tags or not isinstance(tags, list):
            tags = ["æœªåˆ†ç±»"]
            
        for tag in tags:
            # ç¡®ä¿tagæ˜¯æœ‰æ•ˆå­—ç¬¦ä¸²
            if not tag or str(tag).lower() == 'nan':
                tag = "æœªåˆ†ç±»"
            tag = str(tag).strip()
            
            folder_path = f"data_by_type/{tag}"
            self.ensure_directory(folder_path)
            
            filename = f"{tag}-{safe_nick_name}-ç¬¬{page_num}é¡µ-{crawl_date}.csv"
            filepath = os.path.join(folder_path, filename)
            
            # æ¸…ç†æ•°æ®ä¸­çš„NaNå€¼
            clean_data = {}
            for key, value in author_data.items():
                if value is None or str(value).lower() == 'nan':
                    if key in ['ç²‰ä¸æ•° (follower)', 'è¿‘30å¤©å¹³å‡æ’­æ”¾é‡ (vv_median_30d)', 'æŠ¥ä»· (price)', 'æ˜Ÿå›¾æŒ‡æ•° (star_index)', 'é¡µç ']:
                        clean_data[key] = 0
                    elif key == 'è¿‘30å¤©äº’åŠ¨ç‡ (interact_rate_within_30d)':
                        clean_data[key] = 0.0
                    else:
                        clean_data[key] = ''
                else:
                    clean_data[key] = value
            
            # å†™å…¥å•æ¡è®°å½•
            try:
                with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=clean_data.keys())
                    writer.writeheader()
                    writer.writerow(clean_data)
                    
                self.logger.debug(f"ä¿å­˜è¾¾äººæ•°æ®: {filepath}")
                
            except Exception as e:
                self.logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
                
    def crawl_pages(self, start_page: int, end_page: int, task_name: str = None, 
                   domain_filter: str = None, resume_from: int = None):
        """çˆ¬å–æŒ‡å®šé¡µé¢èŒƒå›´çš„æ•°æ®"""
        
        # åˆ›å»ºä»»åŠ¡
        if not task_name:
            task_name = f"çˆ¬å–_{start_page}åˆ°{end_page}é¡µ_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
        self.create_task(task_name, start_page, end_page, domain_filter)
        
        crawl_date = date.today().strftime("%Y%m%d")
        total_authors = 0
        success_authors = 0
        failed_authors = 0
        
        # ç¡®å®šèµ·å§‹é¡µç ï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰
        actual_start_page = resume_from if resume_from else start_page
        
        self.logger.info(f"å¼€å§‹çˆ¬å–ä»»åŠ¡: {task_name}")
        self.logger.info(f"é¡µé¢èŒƒå›´: {actual_start_page} - {end_page}")
        
        try:
            for page in range(actual_start_page, end_page + 1):
                # åŠ è½½è´¦å·é…ç½®
                if not self.current_account:
                    account = self.load_account_config()
                    if not account:
                        self.logger.error("æ— å¯ç”¨è´¦å·ï¼Œåœæ­¢çˆ¬å–")
                        break
                        
                self.logger.info(f"ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...")
                
                # è·å–æ•°æ®
                raw_data = self.fetch_daren_data_by_page(page)
                
                if not raw_data or raw_data.get('base_resp', {}).get('status_code') != 0:
                    self.logger.warning(f"ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥")
                    failed_authors += 1
                    continue
                    
                # è§£ææ•°æ®
                authors = raw_data.get('authors', [])
                page_success = 0
                
                for author in authors:
                    try:
                        parsed = self.parse_author_data(author, page)
                        
                        # å¦‚æœè®¾ç½®äº†ç±»å‹ç­›é€‰ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…
                        if domain_filter:
                            author_tags = parsed.get("è¾¾äººç±»å‹ (tags)", [])
                            filter_tags = [tag.strip() for tag in domain_filter.split(',')]
                            if not any(tag in author_tags for tag in filter_tags):
                                continue
                                
                        self.save_author_to_type_folder(parsed, page, crawl_date)
                        page_success += 1
                        success_authors += 1
                        
                    except Exception as e:
                        self.logger.error(f"å¤„ç†è¾¾äººæ•°æ®å¤±è´¥: {e}")
                        failed_authors += 1
                        
                total_authors += len(authors)
                
                # æ›´æ–°ä»»åŠ¡è¿›åº¦
                self.update_task_progress(page, total_authors, success_authors, failed_authors)
                
                self.logger.info(f"âœ… ç¬¬ {page} é¡µå¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {page_success}, æ€»è®¡: {len(authors)}")
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
                has_more = raw_data.get('pagination', {}).get('has_more', False)
                if not has_more:
                    self.logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œåœæ­¢çˆ¬å–")
                    break
                    
                # éšæœºå»¶è¿Ÿé˜²åçˆ¬
                delay = random.uniform(
                    float(os.getenv('CRAWL_DELAY_MIN', 1.0)),
                    float(os.getenv('CRAWL_DELAY_MAX', 3.0))
                )
                time.sleep(delay)
                
            # å®Œæˆä»»åŠ¡
            self.complete_task('completed')
            self.logger.info(f"ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆï¼æ€»è®¡å¤„ç† {total_authors} ä½è¾¾äººï¼ŒæˆåŠŸ {success_authors}ï¼Œå¤±è´¥ {failed_authors}")
            
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            self.complete_task('paused', 'ç”¨æˆ·ä¸­æ–­')
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.complete_task('failed', str(e))
            
    def resume_task(self, task_id: int):
        """æ¢å¤æœªå®Œæˆçš„ä»»åŠ¡"""
        conn = self.get_db_connection()
        if not conn:
            return
            
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    "SELECT * FROM daoren_task_log WHERE id = %s",
                    (task_id,)
                )
                task = cursor.fetchone()
                
                if not task:
                    self.logger.error(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
                    return
                    
                if task['status'] not in ['paused', 'failed']:
                    self.logger.error(f"ä»»åŠ¡ {task_id} çŠ¶æ€ä¸º {task['status']}ï¼Œæ— æ³•æ¢å¤")
                    return
                    
                self.logger.info(f"æ¢å¤ä»»åŠ¡: {task['task_name']}")
                
                # ä»å½“å‰é¡µç +1å¼€å§‹ç»§ç»­
                resume_page = task['current_page'] + 1 if task['current_page'] > 0 else task['start_page']
                
                self.task_id = task_id
                self.crawl_pages(
                    task['start_page'], 
                    task['end_page'],
                    task['task_name'],
                    task['domain_filter'],
                    resume_page
                )
                
        except Exception as e:
            self.logger.error(f"æ¢å¤ä»»åŠ¡å¤±è´¥: {e}")
        finally:
            conn.close()


def main():
    parser = argparse.ArgumentParser(description='è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿ')
    parser.add_argument('--start', type=int, default=1, help='èµ·å§‹é¡µç ')
    parser.add_argument('--end', type=int, default=5, help='ç»“æŸé¡µç ')
    parser.add_argument('--task-name', type=str, help='ä»»åŠ¡åç§°')
    parser.add_argument('--domain-filter', type=str, help='ç±»å‹ç­›é€‰ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œå¦‚: ç¾å¦†,æ—¶å°š')
    parser.add_argument('--resume-task', type=int, help='æ¢å¤æŒ‡å®šIDçš„ä»»åŠ¡')
    parser.add_argument('--account-id', type=int, help='æŒ‡å®šä½¿ç”¨çš„è´¦å·ID')
    
    args = parser.parse_args()
    
    crawler = DarenCrawler()
    
    if args.resume_task:
        crawler.resume_task(args.resume_task)
    else:
        # åŠ è½½æŒ‡å®šè´¦å·æˆ–è‡ªåŠ¨é€‰æ‹©
        crawler.load_account_config(args.account_id)
        
        crawler.crawl_pages(
            args.start, 
            args.end, 
            args.task_name,
            args.domain_filter
        )


if __name__ == "__main__":
    main()