# -*- coding: utf-8 -*-
"""
è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿ - å¤šçº¿ç¨‹å¼‚æ­¥çˆ¬è™«è„šæœ¬
æ”¯æŒå¹¶å‘çˆ¬å–ã€çº¿ç¨‹æ± ç®¡ç†ã€å¼‚æ­¥æ•°æ®å¤„ç†
åˆ›å»ºæ—¶é—´: 2025-01-23
"""

import os
import json
import time
import csv
import argparse
import logging
import random
import threading
from datetime import date, datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue, Empty
import requests
from dotenv import load_dotenv
import pymysql
from pymysql.cursors import DictCursor
from monitor import get_monitor, start_monitoring, stop_monitoring, record_request

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class AsyncDarenCrawler:
    def __init__(self, max_workers: int = 5):
        self.base_url = "https://agent.oceanengine.com/star/mirror/gw/api/gsearch/search_for_author_square"
        self.max_workers = max_workers
        self.setup_logging()
        self.setup_directories()
        self.db_config = self.get_db_config()
        self.current_accounts = []
        self.task_id = None
        
        # çº¿ç¨‹å®‰å…¨é”
        self.db_lock = threading.Lock()
        self.file_lock = threading.Lock()
        self.stats_lock = threading.Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_authors = 0
        self.success_authors = 0
        self.failed_authors = 0
        
        # æ€§èƒ½ç›‘æ§
        self.monitor = get_monitor()
        
        # é”™è¯¯å¤„ç†é…ç½®
        self.max_consecutive_errors = int(os.getenv('MAX_CONSECUTIVE_ERRORS', 5))
        self.error_cooldown = int(os.getenv('ERROR_COOLDOWN_SECONDS', 60))
        self.consecutive_errors = 0
        self.last_error_time = None
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        log_filename = f"async_crawler_{date.today().strftime('%Y%m%d')}.log"
        log_path = log_dir / log_filename
        
        logging.basicConfig(
            level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
            format=os.getenv('LOG_FORMAT', '%(asctime)s - %(name)s - %(levelname)s - [%(threadName)s] - %(message)s'),
            handlers=[
                logging.FileHandler(log_path, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        directories = [
            "logs",
            "data_by_type", 
            "exports",
            "temp",
            "backups"
        ]
        for dir_name in directories:
            Path(dir_name).mkdir(exist_ok=True)
            
    def get_db_config(self):
        """è·å–æ•°æ®åº“é…ç½®"""
        return {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 3306)),
            'user': os.getenv('DB_USER', 'root'),
            'password': os.getenv('DB_PASSWORD', ''),
            'database': os.getenv('DB_NAME', 'influencer_crawler'),
            'charset': 'utf8mb4'
        }
        
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        try:
            return pymysql.connect(**self.db_config, cursorclass=DictCursor)
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return None
            
    def load_all_accounts(self, account_id: int = None):
        """åŠ è½½æ‰€æœ‰å¯ç”¨è´¦å·æˆ–æŒ‡å®šè´¦å·"""
        conn = self.get_db_connection()
        if not conn:
            return []
            
        try:
            with conn.cursor() as cursor:
                if account_id:
                    # åŠ è½½æŒ‡å®šè´¦å·
                    cursor.execute("""
                        SELECT * FROM daoren_account 
                        WHERE id = %s AND status = 'active' 
                        AND (cooldown_until IS NULL OR cooldown_until < NOW())
                    """, (account_id,))
                    self.logger.info(f"å°è¯•åŠ è½½æŒ‡å®šè´¦å· ID: {account_id}")
                else:
                    # åŠ è½½æ‰€æœ‰å¯ç”¨è´¦å·
                    cursor.execute("""
                        SELECT * FROM daoren_account 
                        WHERE status = 'active' 
                        AND (cooldown_until IS NULL OR cooldown_until < NOW())
                        ORDER BY last_used_at ASC, success_count ASC
                    """)
                
                accounts = cursor.fetchall()
                self.current_accounts = accounts
                
                if account_id and not accounts:
                    self.logger.error(f"æŒ‡å®šçš„è´¦å· ID {account_id} ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨")
                    return []
                    
                self.logger.info(f"åŠ è½½äº† {len(accounts)} ä¸ªå¯ç”¨è´¦å·")
                return accounts
                
        except Exception as e:
            self.logger.error(f"åŠ è½½è´¦å·é…ç½®å¤±è´¥: {e}")
            return []
        finally:
            conn.close()
            
    def get_account_for_thread(self, thread_id: int):
        """ä¸ºçº¿ç¨‹åˆ†é…è´¦å·"""
        if not self.current_accounts:
            return None
        # è½®è¯¢åˆ†é…è´¦å·
        account_index = thread_id % len(self.current_accounts)
        return self.current_accounts[account_index]
        
    def get_headers(self, account: Dict[str, Any]):
        """è·å–è¯·æ±‚å¤´"""
        if account and account.get('headers'):
            try:
                custom_headers = json.loads(account['headers'])
                return custom_headers
            except:
                pass
                
        # é»˜è®¤è¯·æ±‚å¤´
        return {
            'accept': 'application/json, text/plain, */*',
            'accept-language': 'zh-CN,zh;q=0.9',
            'agw-js-conv': 'str',
            'content-type': 'application/json',
            'origin': 'https://agent.oceanengine.com',
            'priority': 'u=1, i',
            'referer': 'https://agent.oceanengine.com/admin/star-agent/vue2/market',
            'sec-ch-ua': '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',
            'x-login-source': '1',
            'x-tt-possess-scene': '2',
            'x-tt-possess-star-id': '1843934177451019',
            'cookie': account.get('cookies', '') if account else ''
        }
        
    def create_task(self, task_name: str, start_page: int, end_page: int, domain_filter: str = None):
        """åˆ›å»ºçˆ¬å–ä»»åŠ¡"""
        with self.db_lock:
            conn = self.get_db_connection()
            if not conn:
                return None
                
            try:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        INSERT INTO daoren_task_log 
                        (task_name, domain_filter, start_page, end_page, status, start_time)
                        VALUES (%s, %s, %s, %s, 'running', NOW())
                    """, (task_name, domain_filter, start_page, end_page))
                    
                    task_id = cursor.lastrowid
                    conn.commit()
                    self.task_id = task_id
                    self.logger.info(f"åˆ›å»ºä»»åŠ¡ ID: {task_id}, åç§°: {task_name}")
                    return task_id
                    
            except Exception as e:
                self.logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
                return None
            finally:
                conn.close()
                
    def update_task_progress(self, current_page: int):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if not self.task_id:
            return
            
        with self.db_lock:
            conn = self.get_db_connection()
            if not conn:
                return
                
            try:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE daoren_task_log 
                        SET current_page = %s, total_authors = %s, success_authors = %s, failed_authors = %s
                        WHERE id = %s
                    """, (current_page, self.total_authors, self.success_authors, self.failed_authors, self.task_id))
                    
                    conn.commit()
                    
            except Exception as e:
                self.logger.error(f"æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")
            finally:
                conn.close()
                
    def complete_task(self, status: str = 'completed', error_message: str = None):
        """å®Œæˆä»»åŠ¡"""
        if not self.task_id:
            return
            
        with self.db_lock:
            conn = self.get_db_connection()
            if not conn:
                return
                
            try:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE daoren_task_log 
                        SET status = %s, end_time = NOW(), error_message = %s
                        WHERE id = %s
                    """, (status, error_message, self.task_id))
                    
                    conn.commit()
                    self.logger.info(f"ä»»åŠ¡ {self.task_id} çŠ¶æ€æ›´æ–°ä¸º: {status}")
                    
            except Exception as e:
                self.logger.error(f"å®Œæˆä»»åŠ¡å¤±è´¥: {e}")
            finally:
                conn.close()
                
    def fetch_page_data(self, page_num: int, thread_id: int) -> Dict[str, Any]:
        """è·å–å•é¡µæ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        account = self.get_account_for_thread(thread_id)
        if not account:
            self.logger.error(f"çº¿ç¨‹ {thread_id} æ— å¯ç”¨è´¦å·")
            record_request(False, 0, 'no_account')
            return {}
            
        headers = self.get_headers(account)
        
        payload = {
            "scene_param": {
                "platform_source": 1,
                "search_scene": 1,
                "display_scene": 1,
                "marketing_target": 1,
                "task_category": 1,
                "first_industry_id": 0,
                "task_status": 3
            },
            "search_param": {
                "seach_type": 2
            },
            "sort_param": {
                "sort_type": 2,
                "sort_field": {
                    "field_name": "score"
                }
            },
            "page_param": {
                "page": page_num,
                "limit": 20
            },
            "attribute_filter": [
                {
                    "field": {
                        "field_name": "price_by_video_type__ge",
                        "rel_id": "2"
                    },
                    "field_value": "0"
                }
            ]
        }
        
        start_time = time.time()
        
        try:
            self.logger.info(f"[çº¿ç¨‹{thread_id}] æ­£åœ¨è¯·æ±‚ç¬¬ {page_num} é¡µæ•°æ®...")
            
            response = requests.post(
                self.base_url, 
                headers=headers, 
                json=payload, 
                timeout=int(os.getenv('API_TIMEOUT', 30))
            )
            response.raise_for_status()
            
            response_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è®°å½•æˆåŠŸè¯·æ±‚
            record_request(True, response_time)
            
            # æ›´æ–°è´¦å·æˆåŠŸè®¡æ•°
            self.update_account_stats(account['id'], success=True)
            
            # é‡ç½®è¿ç»­é”™è¯¯è®¡æ•°
            self.consecutive_errors = 0
            
            return response.json()
            
        except requests.exceptions.RequestException as e:
            response_time = (time.time() - start_time) * 1000
            error_type = self._classify_error(e)
            
            self.logger.error(f"[çº¿ç¨‹{thread_id}] è¯·æ±‚ç¬¬ {page_num} é¡µå¤±è´¥: {e}")
            
            # è®°å½•å¤±è´¥è¯·æ±‚
            record_request(False, response_time, error_type)
            
            # æ›´æ–°è´¦å·å¤±è´¥è®¡æ•°
            self.update_account_stats(account['id'], success=False)
            
            # å¤„ç†è¿ç»­é”™è¯¯
            self._handle_consecutive_error()
            
            return {}
            
    def _classify_error(self, error: Exception) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_str = str(error).lower()
        if 'timeout' in error_str:
            return 'timeout'
        elif 'connection' in error_str:
            return 'connection_error'
        elif '401' in error_str or '403' in error_str:
            return 'auth_error'
        elif '429' in error_str:
            return 'rate_limit'
        elif '500' in error_str or '502' in error_str or '503' in error_str:
            return 'server_error'
        else:
            return 'unknown_error'
            
    def _handle_consecutive_error(self):
        """å¤„ç†è¿ç»­é”™è¯¯"""
        self.consecutive_errors += 1
        self.last_error_time = time.time()
        
        if self.consecutive_errors >= self.max_consecutive_errors:
            self.logger.warning(f"è¿ç»­ {self.consecutive_errors} æ¬¡é”™è¯¯ï¼Œæš‚åœ {self.error_cooldown} ç§’")
            time.sleep(self.error_cooldown)
            self.consecutive_errors = 0
            
    def update_account_stats(self, account_id: int, success: bool):
        """æ›´æ–°è´¦å·ç»Ÿè®¡ä¿¡æ¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.db_lock:
            conn = self.get_db_connection()
            if not conn:
                return
                
            try:
                with conn.cursor() as cursor:
                    if success:
                        cursor.execute(
                            "UPDATE daoren_account SET success_count = success_count + 1 WHERE id = %s",
                            (account_id,)
                        )
                    else:
                        cursor.execute(
                            "UPDATE daoren_account SET failed_count = failed_count + 1 WHERE id = %s",
                            (account_id,)
                        )
                    conn.commit()
                    
            except Exception as e:
                self.logger.error(f"æ›´æ–°è´¦å·ç»Ÿè®¡å¤±è´¥: {e}")
            finally:
                conn.close()
                
    def parse_author_data(self, author: Dict[str, Any], page_num: int) -> Dict[str, Any]:
        """è§£æå•ä¸ªè¾¾äººçš„æ•°æ® - å®Œæ•´ç‰ˆæœ¬ï¼ŒåŒ…å«æ‰€æœ‰æœ‰ä»·å€¼å­—æ®µ"""
        
        # æ·»åŠ è°ƒè¯•è¾“å‡º
        print(f"\n=== è°ƒè¯•ä¿¡æ¯ - ç¬¬{page_num}é¡µè¾¾äººæ•°æ® ===")
        print(f"åŸå§‹authoræ•°æ®ç»“æ„: {json.dumps(author, ensure_ascii=False, indent=2)[:1000]}...")
        
        attr_data = author.get('attribute_datas', {})
        print(f"attribute_dataså­˜åœ¨: {bool(attr_data)}")
        if attr_data:
            print(f"nick_nameå­—æ®µ: {attr_data.get('nick_name', 'ä¸å­˜åœ¨')}")
            print(f"followerå­—æ®µ: {attr_data.get('follower', 'ä¸å­˜åœ¨')}")
            print(f"tags_relationå­—æ®µ: {attr_data.get('tags_relation', 'ä¸å­˜åœ¨')}")
        print("=" * 50)
        
        # å®‰å…¨è·å–task_infoï¼Œé¿å…list index out of rangeé”™è¯¯
        task_infos = author.get('task_infos', [])
        task_info = task_infos[0] if task_infos else {}
        
        # å®‰å…¨è·å–price_infoï¼Œé¿å…list index out of rangeé”™è¯¯
        price_infos = task_info.get('price_infos', [])
        
        # å®‰å…¨è·å–æ•°å€¼ï¼Œé¿å…Noneå’ŒNaN
        def safe_get_number(data, key, default=0):
            value = data.get(key, default)
            if value is None or str(value).lower() == 'nan':
                return default
            try:
                return int(value) if isinstance(value, (int, float)) and value == int(value) else float(value)
            except (ValueError, TypeError):
                return default
                
        def safe_get_string(data, key, default=''):
            value = data.get(key, default)
            if value is None or str(value).lower() == 'nan':
                return default
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å»é™¤ç©ºç™½ï¼Œå¦‚æœç»“æœä¸ºç©ºåˆ™è¿”å›é»˜è®¤å€¼
            str_value = str(value).strip()
            if not str_value or str_value == '0':
                return default if default else 'æœªçŸ¥'
            return str_value
            
        def safe_get_json_string(data, key, default=''):
            """å®‰å…¨è·å–JSONå­—ç¬¦ä¸²å¹¶æ ¼å¼åŒ–"""
            value = data.get(key, default)
            if value is None or str(value).lower() == 'nan':
                return default
            try:
                if isinstance(value, str):
                    # å°è¯•è§£æJSONå¹¶é‡æ–°æ ¼å¼åŒ–
                    parsed = json.loads(value)
                    return json.dumps(parsed, ensure_ascii=False, separators=(',', ':'))
                elif isinstance(value, (list, dict)):
                    return json.dumps(value, ensure_ascii=False, separators=(',', ':'))
                else:
                    return str(value)
            except:
                return str(value) if value else default
        
        # æå–è¾¾äººç±»å‹
        tags_relation_str = attr_data.get('tags_relation', '{}')
        è¾¾äººç±»å‹åˆ—è¡¨ = []
        try:
            # å®‰å…¨å¤„ç†tags_relationå­—æ®µ
            if tags_relation_str is None or str(tags_relation_str).lower() == 'nan':
                tags_relation_str = '{}'
            elif not isinstance(tags_relation_str, str):
                tags_relation_str = str(tags_relation_str)
                
            tags_relation_dict = json.loads(tags_relation_str)
            è¾¾äººç±»å‹åˆ—è¡¨ = [tag for tag in tags_relation_dict.keys() if tag and str(tag).strip()]
        except (json.JSONDecodeError, TypeError, AttributeError):
            è¾¾äººç±»å‹åˆ—è¡¨ = []
        
        # è·å–æ‰€æœ‰ä»·æ ¼ä¿¡æ¯
        price_1_20 = safe_get_number(attr_data, 'price_1_20', 0)
        price_20_60 = safe_get_number(attr_data, 'price_20_60', 0)
        price_60 = safe_get_number(attr_data, 'price_60', 0)
        
        # ä»price_infosä¸­è·å–æ›´å¤šä»·æ ¼ä¿¡æ¯
        all_prices = []
        video_types = []
        for price_info in price_infos:
            price = safe_get_number(price_info, 'price', 0)
            video_type = safe_get_number(price_info, 'video_type', 0)
            if price > 0:
                all_prices.append(price)
                video_types.append(video_type)
        
        # è®¡ç®—ä»·æ ¼ç»Ÿè®¡
        min_price = min(all_prices) if all_prices else 0
        max_price = max(all_prices) if all_prices else 0
        avg_price = sum(all_prices) / len(all_prices) if all_prices else 0
        
        # æ„å»ºå®Œæ•´çš„æ•°æ®å­—å…¸
        parsed_data = {
            # === åŸºç¡€èº«ä»½ä¿¡æ¯ ===
            "è¾¾äººID (star_id)": safe_get_string(author, 'star_id'),
            "æ˜µç§° (nick_name)": safe_get_string(attr_data, 'nick_name'),
            "æ ¸å¿ƒç”¨æˆ·ID (core_user_id)": safe_get_string(attr_data, 'core_user_id'),
            "å¤´åƒé“¾æ¥ (avatar_uri)": safe_get_string(attr_data, 'avatar_uri'),
            "æ€§åˆ« (gender)": safe_get_number(attr_data, 'gender'),
            "æ‰€åœ¨åœ° (city)": safe_get_string(attr_data, 'city'),
            "çœä»½ (province)": safe_get_string(attr_data, 'province'),
            "è¾¾äººç±»å‹ (author_type)": safe_get_number(attr_data, 'author_type'),
            "è´¦å·çŠ¶æ€ (author_status)": safe_get_number(attr_data, 'author_status'),
            "è¾¾äººç­‰çº§ (grade)": safe_get_number(attr_data, 'grade'),
            
            # === ç²‰ä¸ä¸å½±å“åŠ›æ•°æ® ===
            "ç²‰ä¸æ•° (follower)": safe_get_number(attr_data, 'follower'),
            "15å¤©ç²‰ä¸å¢é•¿æ•° (fans_increment_within_15d)": safe_get_number(attr_data, 'fans_increment_within_15d'),
            "30å¤©ç²‰ä¸å¢é•¿æ•° (fans_increment_within_30d)": safe_get_number(attr_data, 'fans_increment_within_30d'),
            "15å¤©ç²‰ä¸å¢é•¿ç‡ (fans_increment_rate_within_15d)": safe_get_number(attr_data, 'fans_increment_rate_within_15d'),
            "è¿‘30å¤©äº’åŠ¨ç‡ (interact_rate_within_30d)": safe_get_number(attr_data, 'interact_rate_within_30d'),
            "30å¤©äº’åŠ¨ä¸­ä½æ•° (interaction_median_30d)": safe_get_number(attr_data, 'interaction_median_30d'),
            "30å¤©æ’­æ”¾å®Œæˆç‡ (play_over_rate_within_30d)": safe_get_number(attr_data, 'play_over_rate_within_30d'),
            "è¿‘30å¤©å¹³å‡æ’­æ”¾é‡ (vv_median_30d)": safe_get_number(attr_data, 'vv_median_30d'),
            
            # === å†…å®¹åˆ›ä½œæ•°æ® ===
            "30å¤©æ˜Ÿå›¾è§†é¢‘æ•°é‡ (star_item_count_within_30d)": safe_get_number(attr_data, 'star_item_count_within_30d'),
            "90å¤©æ˜Ÿå›¾è§†é¢‘æ€»æ•° (star_video_cnt_90d)": safe_get_number(attr_data, 'star_video_cnt_90d'),
            "90å¤©æ˜Ÿå›¾è§†é¢‘äº’åŠ¨ç‡ (star_video_interact_rate_90d)": safe_get_number(attr_data, 'star_video_interact_rate_90d'),
            "90å¤©æ˜Ÿå›¾è§†é¢‘å®Œæ’­ç‡ (star_video_finish_vv_rate_90d)": safe_get_number(attr_data, 'star_video_finish_vv_rate_90d'),
            "90å¤©æ˜Ÿå›¾è§†é¢‘æ’­æ”¾ä¸­ä½æ•° (star_video_median_vv_90d)": safe_get_number(attr_data, 'star_video_median_vv_90d'),
            "å†…å®¹ä¸»é¢˜æ ‡ç­¾ (content_theme_labels_180d)": safe_get_json_string(attr_data, 'content_theme_labels_180d'),
            "è¾¾äººæ ‡ç­¾å…³ç³» (tags_relation)": safe_get_json_string(attr_data, 'tags_relation'),
            "è¾¾äººç±»å‹ (tags)": è¾¾äººç±»å‹åˆ—è¡¨,
            
            # === å•†ä¸šä»·å€¼æ•°æ® ===
            "1-20ç§’è§†é¢‘æŠ¥ä»· (price_1_20)": price_1_20,
            "20-60ç§’è§†é¢‘æŠ¥ä»· (price_20_60)": price_20_60,
            "60ç§’ä»¥ä¸Šè§†é¢‘æŠ¥ä»· (price_60)": price_60,
            "æŒ‡æ´¾ä»»åŠ¡ä»·æ ¼åŒºé—´ (assign_task_price_list)": safe_get_string(attr_data, 'assign_task_price_list'),
            "é¢„æœŸæ’­æ”¾é‡ (expected_play_num)": safe_get_number(attr_data, 'expected_play_num'),
            "é¢„æœŸè‡ªç„¶æ’­æ”¾é‡ (expected_natural_play_num)": safe_get_number(attr_data, 'expected_natural_play_num'),
            "æ˜Ÿå›¾æŒ‡æ•° (star_index)": safe_get_number(attr_data, 'star_index'),
            
            # === CPMæˆæœ¬æ•°æ® ===
            "1-20ç§’é¢„æœŸCPM (prospective_1_20_cpm)": safe_get_number(attr_data, 'prospective_1_20_cpm'),
            "20-60ç§’é¢„æœŸCPM (prospective_20_60_cpm)": safe_get_number(attr_data, 'prospective_20_60_cpm'),
            "60ç§’ä»¥ä¸Šé¢„æœŸCPM (prospective_60_cpm)": safe_get_number(attr_data, 'prospective_60_cpm'),
            "æ¨å¹¿1-20ç§’é¢„æœŸCPM (promotion_prospective_1_20_cpm)": safe_get_number(attr_data, 'promotion_prospective_1_20_cpm'),
            "æ¨å¹¿20-60ç§’é¢„æœŸCPM (promotion_prospective_20_60_cpm)": safe_get_number(attr_data, 'promotion_prospective_20_60_cpm'),
            "æ¨å¹¿60ç§’ä»¥ä¸Šé¢„æœŸCPM (promotion_prospective_60_cpm)": safe_get_number(attr_data, 'promotion_prospective_60_cpm'),
            "æ¨å¹¿é¢„æœŸæ’­æ”¾é‡ (promotion_prospective_vv)": safe_get_number(attr_data, 'promotion_prospective_vv'),
            
            # === ç”µå•†æ•°æ® ===
            "ç”µå•†åŠŸèƒ½å¼€é€š (e_commerce_enable)": safe_get_number(attr_data, 'e_commerce_enable'),
            "ç”µå•†ç­‰çº§ (author_ecom_level)": safe_get_string(attr_data, 'author_ecom_level'),
            "30å¤©GMVèŒƒå›´ (ecom_gmv_30d_range)": safe_get_string(attr_data, 'ecom_gmv_30d_range'),
            "30å¤©å¹³å‡è®¢å•ä»·å€¼ (ecom_avg_order_value_30d_range)": safe_get_string(attr_data, 'ecom_avg_order_value_30d_range'),
            "30å¤©æ¯›åˆ©ç‡èŒƒå›´ (ecom_gpm_30d_range)": safe_get_string(attr_data, 'ecom_gpm_30d_range'),
            "30å¤©å¸¦è´§è§†é¢‘æ•° (ecom_video_product_num_30d)": safe_get_number(attr_data, 'ecom_video_product_num_30d'),
            "30å¤©æ˜Ÿå›¾å¸¦è´§è§†é¢‘æ•° (star_ecom_video_num_30d)": safe_get_number(attr_data, 'star_ecom_video_num_30d'),
            
            # === æ€§èƒ½æŒ‡æ ‡ ===
            "é“¾æ¥è½¬åŒ–æŒ‡æ•° (link_convert_index)": safe_get_number(attr_data, 'link_convert_index'),
            "è¡Œä¸šé“¾æ¥è½¬åŒ–æŒ‡æ•° (link_convert_index_by_industry)": safe_get_number(attr_data, 'link_convert_index_by_industry'),
            "è´­ç‰©æŒ‡æ•° (link_shopping_index)": safe_get_number(attr_data, 'link_shopping_index'),
            "ä¼ æ’­æŒ‡æ•° (link_spread_index)": safe_get_number(attr_data, 'link_spread_index'),
            "è¡Œä¸šä¼ æ’­æŒ‡æ•° (link_spread_index_by_industry)": safe_get_number(attr_data, 'link_spread_index_by_industry'),
            "é“¾æ¥æ˜Ÿå›¾æŒ‡æ•° (link_star_index)": safe_get_number(attr_data, 'link_star_index'),
            "è¡Œä¸šé“¾æ¥æ˜Ÿå›¾æŒ‡æ•° (link_star_index_by_industry)": safe_get_number(attr_data, 'link_star_index_by_industry'),
            "è¡Œä¸šæ¨èæŒ‡æ•° (link_recommend_index_by_industry)": safe_get_number(attr_data, 'link_recommend_index_by_industry'),
            "æœç´¢åè§‚çœ‹æŒ‡æ•° (search_after_view_index_by_industry)": safe_get_number(attr_data, 'search_after_view_index_by_industry'),
            
            # === è®¤è¯ä¸ç­‰çº§ ===
            "ä¼˜è´¨è¾¾äºº (is_excellenct_author)": safe_get_number(attr_data, 'is_excellenct_author'),
            "æ˜Ÿå›¾ä¼˜è´¨è¾¾äºº (star_excellent_author)": safe_get_number(attr_data, 'star_excellent_author'),
            "å¤´åƒæ¡†ç­‰çº§ (author_avatar_frame_icon)": safe_get_string(attr_data, 'author_avatar_frame_icon'),
            "é»‘é©¬è¾¾äºº (is_black_horse_author)": safe_get_number(attr_data, 'is_black_horse_author'),
            "å…±åˆ›è¾¾äºº (is_cocreate_author)": safe_get_number(attr_data, 'is_cocreate_author'),
            "CPMé¡¹ç›®è¾¾äºº (is_cpm_project_author)": safe_get_number(attr_data, 'is_cpm_project_author'),
            "çŸ­å‰§è¾¾äºº (is_short_drama)": safe_get_number(attr_data, 'is_short_drama'),
            "æ˜Ÿå›¾ç§ä¿¡è¾¾äºº (star_whispers_author)": safe_get_number(attr_data, 'star_whispers_author'),
            "æœ¬åœ°ä½é—¨æ§›è¾¾äºº (local_lower_threshold_author)": safe_get_number(attr_data, 'local_lower_threshold_author'),
            
            # === å…¶ä»–æ•°æ® ===
            "çˆ†æ–‡ç‡ (burst_text_rate)": safe_get_number(attr_data, 'burst_text_rate'),
            "å“ç‰Œæå‡æ’­æ”¾é‡ (brand_boost_vv)": safe_get_number(attr_data, 'brand_boost_vv'),
            "è§†é¢‘å“ç‰Œæå‡ (video_brand_boost)": safe_get_number(attr_data, 'video_brand_boost'),
            "è§†é¢‘å“ç‰Œæå‡æ’­æ”¾é‡ (video_brand_boost_vv)": safe_get_number(attr_data, 'video_brand_boost_vv'),
            "é¢„æœŸCPA3ç­‰çº§ (expected_cpa3_level)": safe_get_number(attr_data, 'expected_cpa3_level'),
            "æ¸¸æˆç±»å‹ (game_type)": safe_get_string(attr_data, 'game_type'),
            
            # === ç»„ä»¶æ•°æ® ===
            "90å¤©ç»„ä»¶å®‰è£…å®Œæˆæ•° (star_component_install_finish_cnt_90d)": safe_get_number(attr_data, 'star_component_install_finish_cnt_90d'),
            "90å¤©ç»„ä»¶é“¾æ¥ç‚¹å‡»æ•° (star_component_link_click_cnt_90d)": safe_get_number(attr_data, 'star_component_link_click_cnt_90d'),
            "90å¤©è§†é¢‘å®‰è£…>=1æ¬¡æ•° (star_video_install_ge_1_cnt_90d)": safe_get_number(attr_data, 'star_video_install_ge_1_cnt_90d'),
            
            # === çƒ­é—¨è§†é¢‘æ•°æ® ===
            "çƒ­é—¨è§†é¢‘æ•°é‡": len(author.get('items', [])),
            "çƒ­é—¨è§†é¢‘æ€»æ’­æ”¾é‡": sum([safe_get_number(item, 'vv', 0) for item in author.get('items', [])]),
            "çƒ­é—¨è§†é¢‘å¹³å‡æ’­æ”¾é‡": round(sum([safe_get_number(item, 'vv', 0) for item in author.get('items', [])]) / len(author.get('items', [])), 0) if author.get('items') else 0,
            
            # === è¾¾äººç±»å‹æ ‡ç­¾ ===
            "è¾¾äººç±»å‹ (tags)": è¾¾äººç±»å‹åˆ—è¡¨,
            
            # === ç³»ç»Ÿå­—æ®µ ===
            "é¡µç ": page_num,
            "çˆ¬å–æ—¥æœŸ": date.today().strftime('%Y-%m-%d'),
            "çˆ¬å–æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "æ¥æºURL (source_url)": "",
            
            # === åŸå§‹æ•°æ®ï¼ˆç”¨äºè°ƒè¯•å’Œå®Œæ•´æ€§ï¼‰ ===
            "æœ€è¿‘10ä¸ªè§†é¢‘æ•°æ® (last_10_items)": safe_get_json_string(attr_data, 'last_10_items'),
            "çƒ­é—¨è§†é¢‘åˆ—è¡¨ (items)": json.dumps(author.get('items', []), ensure_ascii=False, separators=(',', ':')),
            "ä»»åŠ¡ä¿¡æ¯ (task_infos)": json.dumps(task_infos, ensure_ascii=False, separators=(',', ':'))
        }
        
        return parsed_data
        
    def ensure_directory(self, path: str):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        Path(path).mkdir(parents=True, exist_ok=True)
        
    def save_author_to_type_folder(self, author_data: Dict[str, Any], page_num: int, crawl_date: str):
        """æ ¹æ®è¾¾äººç±»å‹ï¼Œä¿å­˜åˆ°å¯¹åº”æ–‡ä»¶å¤¹ä¸‹çš„ç‹¬ç«‹CSVï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        tags = author_data.get("è¾¾äººç±»å‹ (tags)", [])
        nick_name = author_data.get("æ˜µç§° (nick_name)", "æœªçŸ¥æ˜µç§°")
        
        # ç¡®ä¿nick_nameä¸æ˜¯NaNæˆ–None
        if not nick_name or str(nick_name).lower() == 'nan':
            nick_name = "æœªçŸ¥æ˜µç§°"
        
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        safe_nick_name = "".join(c for c in str(nick_name) if c.isalnum() or c in (' ', '-', '_')).strip()
        if not safe_nick_name:
            safe_nick_name = f"è¾¾äºº_{author_data.get('è¾¾äººID (star_id)', 'unknown')}"
            
        # å¦‚æœæ²¡æœ‰ç±»å‹ï¼Œé»˜è®¤å­˜å…¥ "æœªåˆ†ç±»"
        if not tags or not isinstance(tags, list):
            tags = ["æœªåˆ†ç±»"]
            
        for tag in tags:
            # ç¡®ä¿tagæ˜¯æœ‰æ•ˆå­—ç¬¦ä¸²
            if not tag or str(tag).lower() == 'nan':
                tag = "æœªåˆ†ç±»"
            tag = str(tag).strip()
            
            folder_path = f"data_by_type/{tag}"
            self.ensure_directory(folder_path)
            
            filename = f"{tag}-{safe_nick_name}-ç¬¬{page_num}é¡µ-{crawl_date}.csv"
            filepath = os.path.join(folder_path, filename)
            
            # å®šä¹‰å®Œæ•´çš„å­—æ®µé¡ºåºï¼Œç¡®ä¿CSVè¾“å‡ºçš„å¯è¯»æ€§å’Œä¸€è‡´æ€§
            field_order = [
                # === åŸºç¡€èº«ä»½ä¿¡æ¯ ===
                "è¾¾äººID (star_id)",
                "æ˜µç§° (nick_name)",
                "æ ¸å¿ƒç”¨æˆ·ID (core_user_id)",
                "å¤´åƒé“¾æ¥ (avatar_uri)",
                "æ€§åˆ« (gender)",
                "æ‰€åœ¨åœ° (city)",
                "çœä»½ (province)",
                "è¾¾äººç±»å‹ (author_type)",
                "è´¦å·çŠ¶æ€ (author_status)",
                "è¾¾äººç­‰çº§ (grade)",
                
                # === ç²‰ä¸ä¸å½±å“åŠ›æ•°æ® ===
                "ç²‰ä¸æ•° (follower)",
                "15å¤©ç²‰ä¸å¢é•¿æ•° (fans_increment_within_15d)",
                "30å¤©ç²‰ä¸å¢é•¿æ•° (fans_increment_within_30d)",
                "15å¤©ç²‰ä¸å¢é•¿ç‡ (fans_increment_rate_within_15d)",
                "è¿‘30å¤©äº’åŠ¨ç‡ (interact_rate_within_30d)",
                "30å¤©äº’åŠ¨ä¸­ä½æ•° (interaction_median_30d)",
                "30å¤©æ’­æ”¾å®Œæˆç‡ (play_over_rate_within_30d)",
                "è¿‘30å¤©å¹³å‡æ’­æ”¾é‡ (vv_median_30d)",
                
                # === å†…å®¹åˆ›ä½œæ•°æ® ===
                "30å¤©æ˜Ÿå›¾è§†é¢‘æ•°é‡ (star_item_count_within_30d)",
                "90å¤©æ˜Ÿå›¾è§†é¢‘æ€»æ•° (star_video_cnt_90d)",
                "90å¤©æ˜Ÿå›¾è§†é¢‘äº’åŠ¨ç‡ (star_video_interact_rate_90d)",
                "90å¤©æ˜Ÿå›¾è§†é¢‘å®Œæ’­ç‡ (star_video_finish_vv_rate_90d)",
                "90å¤©æ˜Ÿå›¾è§†é¢‘æ’­æ”¾ä¸­ä½æ•° (star_video_median_vv_90d)",
                "å†…å®¹ä¸»é¢˜æ ‡ç­¾ (content_theme_labels_180d)",
                "è¾¾äººæ ‡ç­¾å…³ç³» (tags_relation)",
                
                # === å•†ä¸šä»·å€¼æ•°æ® ===
                "1-20ç§’è§†é¢‘æŠ¥ä»· (price_1_20)",
                "20-60ç§’è§†é¢‘æŠ¥ä»· (price_20_60)",
                "60ç§’ä»¥ä¸Šè§†é¢‘æŠ¥ä»· (price_60)",
                "æŒ‡æ´¾ä»»åŠ¡ä»·æ ¼åŒºé—´ (assign_task_price_list)",
                "é¢„æœŸæ’­æ”¾é‡ (expected_play_num)",
                "é¢„æœŸè‡ªç„¶æ’­æ”¾é‡ (expected_natural_play_num)",
                "æ˜Ÿå›¾æŒ‡æ•° (star_index)",
                
                # === CPMæˆæœ¬æ•°æ® ===
                "1-20ç§’é¢„æœŸCPM (prospective_1_20_cpm)",
                "20-60ç§’é¢„æœŸCPM (prospective_20_60_cpm)",
                "60ç§’ä»¥ä¸Šé¢„æœŸCPM (prospective_60_cpm)",
                "æ¨å¹¿1-20ç§’é¢„æœŸCPM (promotion_prospective_1_20_cpm)",
                "æ¨å¹¿20-60ç§’é¢„æœŸCPM (promotion_prospective_20_60_cpm)",
                "æ¨å¹¿60ç§’ä»¥ä¸Šé¢„æœŸCPM (promotion_prospective_60_cpm)",
                "æ¨å¹¿é¢„æœŸæ’­æ”¾é‡ (promotion_prospective_vv)",
                
                # === ç”µå•†æ•°æ® ===
                "ç”µå•†åŠŸèƒ½å¼€é€š (e_commerce_enable)",
                "ç”µå•†ç­‰çº§ (author_ecom_level)",
                "30å¤©GMVèŒƒå›´ (ecom_gmv_30d_range)",
                "30å¤©å¹³å‡è®¢å•ä»·å€¼ (ecom_avg_order_value_30d_range)",
                "30å¤©æ¯›åˆ©ç‡èŒƒå›´ (ecom_gpm_30d_range)",
                "30å¤©å¸¦è´§è§†é¢‘æ•° (ecom_video_product_num_30d)",
                "30å¤©æ˜Ÿå›¾å¸¦è´§è§†é¢‘æ•° (star_ecom_video_num_30d)",
                
                # === æ€§èƒ½æŒ‡æ ‡ ===
                "é“¾æ¥è½¬åŒ–æŒ‡æ•° (link_convert_index)",
                "è¡Œä¸šé“¾æ¥è½¬åŒ–æŒ‡æ•° (link_convert_index_by_industry)",
                "è´­ç‰©æŒ‡æ•° (link_shopping_index)",
                "ä¼ æ’­æŒ‡æ•° (link_spread_index)",
                "è¡Œä¸šä¼ æ’­æŒ‡æ•° (link_spread_index_by_industry)",
                "é“¾æ¥æ˜Ÿå›¾æŒ‡æ•° (link_star_index)",
                "è¡Œä¸šé“¾æ¥æ˜Ÿå›¾æŒ‡æ•° (link_star_index_by_industry)",
                "è¡Œä¸šæ¨èæŒ‡æ•° (link_recommend_index_by_industry)",
                "æœç´¢åè§‚çœ‹æŒ‡æ•° (search_after_view_index_by_industry)",
                
                # === è®¤è¯ä¸ç­‰çº§ ===
                "ä¼˜è´¨è¾¾äºº (is_excellenct_author)",
                "æ˜Ÿå›¾ä¼˜è´¨è¾¾äºº (star_excellent_author)",
                "å¤´åƒæ¡†ç­‰çº§ (author_avatar_frame_icon)",
                "é»‘é©¬è¾¾äºº (is_black_horse_author)",
                "å…±åˆ›è¾¾äºº (is_cocreate_author)",
                "CPMé¡¹ç›®è¾¾äºº (is_cpm_project_author)",
                "çŸ­å‰§è¾¾äºº (is_short_drama)",
                "æ˜Ÿå›¾ç§ä¿¡è¾¾äºº (star_whispers_author)",
                "æœ¬åœ°ä½é—¨æ§›è¾¾äºº (local_lower_threshold_author)",
                
                # === å…¶ä»–æ•°æ® ===
                "çˆ†æ–‡ç‡ (burst_text_rate)",
                "å“ç‰Œæå‡æ’­æ”¾é‡ (brand_boost_vv)",
                "è§†é¢‘å“ç‰Œæå‡ (video_brand_boost)",
                "è§†é¢‘å“ç‰Œæå‡æ’­æ”¾é‡ (video_brand_boost_vv)",
                "é¢„æœŸCPA3ç­‰çº§ (expected_cpa3_level)",
                "æ¸¸æˆç±»å‹ (game_type)",
                
                # === ç»„ä»¶æ•°æ® ===
                "90å¤©ç»„ä»¶å®‰è£…å®Œæˆæ•° (star_component_install_finish_cnt_90d)",
                "90å¤©ç»„ä»¶é“¾æ¥ç‚¹å‡»æ•° (star_component_link_click_cnt_90d)",
                "90å¤©è§†é¢‘å®‰è£…>=1æ¬¡æ•° (star_video_install_ge_1_cnt_90d)",
                
                # === çƒ­é—¨è§†é¢‘æ•°æ® ===
                "çƒ­é—¨è§†é¢‘æ•°é‡",
                "çƒ­é—¨è§†é¢‘æ€»æ’­æ”¾é‡",
                "çƒ­é—¨è§†é¢‘å¹³å‡æ’­æ”¾é‡",
                
                # === è¾¾äººç±»å‹æ ‡ç­¾ ===
                "è¾¾äººç±»å‹ (tags)",
                
                # === ç³»ç»Ÿå­—æ®µ ===
                "é¡µç ",
                "çˆ¬å–æ—¥æœŸ",
                "çˆ¬å–æ—¶é—´",
                "æ¥æºURL (source_url)",
                
                # === åŸå§‹æ•°æ®ï¼ˆç”¨äºè°ƒè¯•å’Œå®Œæ•´æ€§ï¼‰ ===
                "æœ€è¿‘10ä¸ªè§†é¢‘æ•°æ® (last_10_items)",
                "çƒ­é—¨è§†é¢‘åˆ—è¡¨ (items)",
                "ä»»åŠ¡ä¿¡æ¯ (task_infos)"
            ]
            
            # æ¸…ç†æ•°æ®ä¸­çš„NaNå€¼å¹¶æŒ‰å­—æ®µé¡ºåºç»„ç»‡æ•°æ®
            clean_data = {}
            
            # æ•°å€¼å­—æ®µå®šä¹‰ï¼ˆç”¨äºNaNå€¼å¤„ç†ï¼‰
            numeric_fields = {
                'ç²‰ä¸æ•° (follower)', '15å¤©ç²‰ä¸å¢é•¿æ•° (fans_increment_within_15d)', 
                '30å¤©ç²‰ä¸å¢é•¿æ•° (fans_increment_within_30d)', '30å¤©äº’åŠ¨ä¸­ä½æ•° (interaction_median_30d)',
                'è¿‘30å¤©å¹³å‡æ’­æ”¾é‡ (vv_median_30d)', '30å¤©æ˜Ÿå›¾è§†é¢‘æ•°é‡ (star_item_count_within_30d)',
                '90å¤©æ˜Ÿå›¾è§†é¢‘æ€»æ•° (star_video_cnt_90d)', '90å¤©æ˜Ÿå›¾è§†é¢‘æ’­æ”¾ä¸­ä½æ•° (star_video_median_vv_90d)',
                '1-20ç§’è§†é¢‘æŠ¥ä»· (price_1_20)', '20-60ç§’è§†é¢‘æŠ¥ä»· (price_20_60)', 
                '60ç§’ä»¥ä¸Šè§†é¢‘æŠ¥ä»· (price_60)', 'é¢„æœŸæ’­æ”¾é‡ (expected_play_num)',
                'é¢„æœŸè‡ªç„¶æ’­æ”¾é‡ (expected_natural_play_num)', 'æ¨å¹¿é¢„æœŸæ’­æ”¾é‡ (promotion_prospective_vv)',
                '30å¤©å¸¦è´§è§†é¢‘æ•° (ecom_video_product_num_30d)', '30å¤©æ˜Ÿå›¾å¸¦è´§è§†é¢‘æ•° (star_ecom_video_num_30d)',
                '90å¤©ç»„ä»¶å®‰è£…å®Œæˆæ•° (star_component_install_finish_cnt_90d)', 
                '90å¤©ç»„ä»¶é“¾æ¥ç‚¹å‡»æ•° (star_component_link_click_cnt_90d)',
                '90å¤©è§†é¢‘å®‰è£…>=1æ¬¡æ•° (star_video_install_ge_1_cnt_90d)', 'å“ç‰Œæå‡æ’­æ”¾é‡ (brand_boost_vv)',
                'è§†é¢‘å“ç‰Œæå‡æ’­æ”¾é‡ (video_brand_boost_vv)', 'é¡µç ', 'æ€§åˆ« (gender)',
                'è¾¾äººç±»å‹ (author_type)', 'è´¦å·çŠ¶æ€ (author_status)', 'è¾¾äººç­‰çº§ (grade)',
                'ç”µå•†åŠŸèƒ½å¼€é€š (e_commerce_enable)', 'ä¼˜è´¨è¾¾äºº (is_excellenct_author)',
                'æ˜Ÿå›¾ä¼˜è´¨è¾¾äºº (star_excellent_author)', 'é»‘é©¬è¾¾äºº (is_black_horse_author)',
                'å…±åˆ›è¾¾äºº (is_cocreate_author)', 'CPMé¡¹ç›®è¾¾äºº (is_cmp_project_author)',
                'çŸ­å‰§è¾¾äºº (is_short_drama)', 'æ˜Ÿå›¾ç§ä¿¡è¾¾äºº (star_whispers_author)',
                'æœ¬åœ°ä½é—¨æ§›è¾¾äºº (local_lower_threshold_author)', 'è§†é¢‘å“ç‰Œæå‡ (video_brand_boost)',
                'é¢„æœŸCPA3ç­‰çº§ (expected_cpa3_level)', 'çƒ­é—¨è§†é¢‘æ•°é‡', 'çƒ­é—¨è§†é¢‘æ€»æ’­æ”¾é‡', 'çƒ­é—¨è§†é¢‘å¹³å‡æ’­æ”¾é‡'
            }
            
            decimal_fields = {
                '15å¤©ç²‰ä¸å¢é•¿ç‡ (fans_increment_rate_within_15d)', 'è¿‘30å¤©äº’åŠ¨ç‡ (interact_rate_within_30d)',
                '30å¤©æ’­æ”¾å®Œæˆç‡ (play_over_rate_within_30d)', '90å¤©æ˜Ÿå›¾è§†é¢‘äº’åŠ¨ç‡ (star_video_interact_rate_90d)',
                '90å¤©æ˜Ÿå›¾è§†é¢‘å®Œæ’­ç‡ (star_video_finish_vv_rate_90d)', 'æ˜Ÿå›¾æŒ‡æ•° (star_index)',
                '1-20ç§’é¢„æœŸCPM (prospective_1_20_cpm)', '20-60ç§’é¢„æœŸCPM (prospective_20_60_cpm)',
                '60ç§’ä»¥ä¸Šé¢„æœŸCPM (prospective_60_cpm)', 'æ¨å¹¿1-20ç§’é¢„æœŸCPM (promotion_prospective_1_20_cpm)',
                'æ¨å¹¿20-60ç§’é¢„æœŸCPM (promotion_prospective_20_60_cpm)', 'æ¨å¹¿60ç§’ä»¥ä¸Šé¢„æœŸCPM (promotion_prospective_60_cpm)',
                'é“¾æ¥è½¬åŒ–æŒ‡æ•° (link_convert_index)', 'è¡Œä¸šé“¾æ¥è½¬åŒ–æŒ‡æ•° (link_convert_index_by_industry)',
                'è´­ç‰©æŒ‡æ•° (link_shopping_index)', 'ä¼ æ’­æŒ‡æ•° (link_spread_index)',
                'è¡Œä¸šä¼ æ’­æŒ‡æ•° (link_spread_index_by_industry)', 'é“¾æ¥æ˜Ÿå›¾æŒ‡æ•° (link_star_index)',
                'è¡Œä¸šé“¾æ¥æ˜Ÿå›¾æŒ‡æ•° (link_star_index_by_industry)', 'è¡Œä¸šæ¨èæŒ‡æ•° (link_recommend_index_by_industry)',
                'æœç´¢åè§‚çœ‹æŒ‡æ•° (search_after_view_index_by_industry)', 'çˆ†æ–‡ç‡ (burst_text_rate)'
            }
            
            # æŒ‰å­—æ®µé¡ºåºå¤„ç†æ•°æ®
            for field in field_order:
                value = author_data.get(field)
                
                if value is None or str(value).lower() == 'nan':
                    if field in numeric_fields:
                        clean_data[field] = 0
                    elif field in decimal_fields:
                        clean_data[field] = 0.0
                    else:
                        clean_data[field] = ''
                else:
                    clean_data[field] = value
            
            # çº¿ç¨‹å®‰å…¨çš„æ–‡ä»¶å†™å…¥
            with self.file_lock:
                try:
                    with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
                        writer = csv.DictWriter(f, fieldnames=field_order)
                        writer.writeheader()
                        writer.writerow(clean_data)
                        
                    self.logger.debug(f"ä¿å­˜è¾¾äººæ•°æ®: {filepath}")
                    
                except Exception as e:
                    self.logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
                    
                    
    def process_page(self, page_num: int, thread_id: int, domain_filter: str = None, crawl_date: str = None) -> tuple:
        """å¤„ç†å•é¡µæ•°æ®ï¼ˆçº¿ç¨‹å·¥ä½œå‡½æ•°ï¼‰"""
        try:
            # è·å–æ•°æ®
            raw_data = self.fetch_page_data(page_num, thread_id)
            
            if not raw_data or raw_data.get('base_resp', {}).get('status_code') != 0:
                self.logger.warning(f"[çº¿ç¨‹{thread_id}] ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥")
                return page_num, 0, 0, 1
                
            # è§£ææ•°æ®
            authors = raw_data.get('authors', [])
            page_success = 0
            page_failed = 0
            
            for author in authors:
                try:
                    parsed = self.parse_author_data(author, page_num)
                    
                    # å¦‚æœè®¾ç½®äº†ç±»å‹ç­›é€‰ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…
                    if domain_filter:
                        author_tags = parsed.get("è¾¾äººç±»å‹ (tags)", [])
                        filter_tags = [tag.strip() for tag in domain_filter.split(',')]
                        if not any(tag in author_tags for tag in filter_tags):
                            continue
                            
                    self.save_author_to_type_folder(parsed, page_num, crawl_date)
                    page_success += 1
                    
                except Exception as e:
                    self.logger.error(f"[çº¿ç¨‹{thread_id}] å¤„ç†è¾¾äººæ•°æ®å¤±è´¥: {e}")
                    page_failed += 1
                    
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            with self.stats_lock:
                self.total_authors += len(authors)
                self.success_authors += page_success
                self.failed_authors += page_failed
                
            self.logger.info(f"[çº¿ç¨‹{thread_id}] âœ… ç¬¬ {page_num} é¡µå¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {page_success}, æ€»è®¡: {len(authors)}")
            
            # éšæœºå»¶è¿Ÿé˜²åçˆ¬
            delay = random.uniform(
                float(os.getenv('CRAWL_DELAY_MIN', 0.5)),
                float(os.getenv('CRAWL_DELAY_MAX', 1.5))
            )
            time.sleep(delay)
            
            return page_num, len(authors), page_success, page_failed
            
        except Exception as e:
            self.logger.error(f"[çº¿ç¨‹{thread_id}] å¤„ç†ç¬¬ {page_num} é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return page_num, 0, 0, 1
            
    def crawl_pages_async(self, start_page: int, end_page: int, task_name: str = None, 
                         domain_filter: str = None, resume_from: int = None, account_id: int = None):
        """å¼‚æ­¥çˆ¬å–æŒ‡å®šé¡µé¢èŒƒå›´çš„æ•°æ®"""
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        start_monitoring()
        
        # åŠ è½½è´¦å·ï¼ˆæ‰€æœ‰è´¦å·æˆ–æŒ‡å®šè´¦å·ï¼‰
        accounts = self.load_all_accounts(account_id)
        if not accounts:
            self.logger.error("æ²¡æœ‰å¯ç”¨è´¦å·ï¼Œæ— æ³•å¼€å§‹çˆ¬å–")
            stop_monitoring()
            return
            
        # åˆ›å»ºä»»åŠ¡
        if not task_name:
            task_name = f"å¼‚æ­¥çˆ¬å–_{start_page}åˆ°{end_page}é¡µ_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
        self.create_task(task_name, start_page, end_page, domain_filter)
        
        crawl_date = date.today().strftime("%Y%m%d")
        
        # ç¡®å®šèµ·å§‹é¡µç ï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰
        actual_start_page = resume_from if resume_from else start_page
        
        self.logger.info(f"å¼€å§‹å¼‚æ­¥çˆ¬å–ä»»åŠ¡: {task_name}")
        self.logger.info(f"é¡µé¢èŒƒå›´: {actual_start_page} - {end_page}")
        self.logger.info(f"ä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹ï¼Œ{len(accounts)} ä¸ªè´¦å·")
        
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œçˆ¬å–ä»»åŠ¡
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰é¡µé¢ä»»åŠ¡
                future_to_page = {
                    executor.submit(self.process_page, page, thread_id % self.max_workers, domain_filter, crawl_date): page
                    for thread_id, page in enumerate(range(actual_start_page, end_page + 1))
                }
                
                completed_pages = 0
                empty_pages = 0
                page_results = {}  # å­˜å‚¨é¡µé¢ç»“æœï¼Œç”¨äºæŒ‰é¡ºåºæ£€æŸ¥è¿ç»­ç©ºé¡µé¢
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_page):
                    page_num = future_to_page[future]
                    try:
                        page_num, total_count, success_count, failed_count = future.result()
                        completed_pages += 1
                        
                        # å­˜å‚¨é¡µé¢ç»“æœ
                        page_results[page_num] = total_count
                        
                        # ç»Ÿè®¡ç©ºé¡µé¢
                        if total_count == 0:
                            empty_pages += 1
                            
                        # æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­çš„ç©ºé¡µé¢ï¼ˆæŒ‰é¡µç é¡ºåºï¼‰
                        consecutive_empty = 0
                        max_page_checked = min(page_results.keys()) if page_results else actual_start_page
                        
                        # ä»æœ€å°é¡µç å¼€å§‹æ£€æŸ¥è¿ç»­ç©ºé¡µé¢
                        for check_page in range(max_page_checked, end_page + 1):
                            if check_page in page_results:
                                if page_results[check_page] == 0:
                                    consecutive_empty += 1
                                else:
                                    consecutive_empty = 0
                                    
                                # å¦‚æœè¿ç»­5é¡µéƒ½æ˜¯ç©ºçš„ï¼Œä¸”å·²ç»çˆ¬å–äº†è‡³å°‘10é¡µï¼Œå¯èƒ½å·²ç»åˆ°è¾¾æ•°æ®æœ«å°¾
                                if consecutive_empty >= 5 and completed_pages >= 10:
                                    self.logger.info(f"è¿ç»­ {consecutive_empty} é¡µæ— æ•°æ®ï¼Œå¯èƒ½å·²åˆ°è¾¾æ•°æ®æœ«å°¾")
                                    # å–æ¶ˆå‰©ä½™ä»»åŠ¡
                                    for remaining_future in future_to_page:
                                        if not remaining_future.done():
                                            remaining_future.cancel()
                                    break
                            else:
                                break  # è¿˜æœ‰é¡µé¢æœªå®Œæˆï¼Œæ— æ³•åˆ¤æ–­è¿ç»­æ€§
                                
                        # å¦‚æœè§¦å‘äº†æå‰åœæ­¢ï¼Œè·³å‡ºä¸»å¾ªç¯
                        if consecutive_empty >= 5 and completed_pages >= 10:
                            break
                            
                        # å®šæœŸæ›´æ–°è¿›åº¦
                        if completed_pages % 10 == 0:
                            self.update_task_progress(page_num)
                            
                    except Exception as e:
                        self.logger.error(f"å¤„ç†é¡µé¢ {page_num} ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        
            # å®Œæˆä»»åŠ¡
            self.complete_task('completed')
            self.logger.info(f"ğŸ‰ å¼‚æ­¥çˆ¬å–ä»»åŠ¡å®Œæˆï¼æ€»è®¡å¤„ç† {self.total_authors} ä½è¾¾äººï¼ŒæˆåŠŸ {self.success_authors}ï¼Œå¤±è´¥ {self.failed_authors}")
            self.logger.info(f"ç©ºé¡µé¢æ•°é‡: {empty_pages}")
            
            # è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
            self._print_performance_report()
            
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            self.complete_task('paused', 'ç”¨æˆ·ä¸­æ–­')
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.complete_task('failed', str(e))
        finally:
            # åœæ­¢æ€§èƒ½ç›‘æ§
            stop_monitoring()
            
    def _print_performance_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        try:
            from monitor import get_performance_report, get_optimization_suggestions
            
            print("\n" + "="*60)
            print("ğŸš€ å¼‚æ­¥çˆ¬è™«æ€§èƒ½æŠ¥å‘Š")
            print("="*60)
            
            report = get_performance_report()
            if 'error' not in report:
                print(f"\nğŸ“Š ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ:")
                print(f"  CPUå¹³å‡ä½¿ç”¨ç‡: {report['CPUä½¿ç”¨ç‡']['å¹³å‡å€¼']}")
                print(f"  å†…å­˜å¹³å‡ä½¿ç”¨ç‡: {report['å†…å­˜ä½¿ç”¨ç‡']['å¹³å‡å€¼']}")
                print(f"  å¹³å‡å“åº”æ—¶é—´: {report['å“åº”æ—¶é—´']['å¹³å‡å€¼']}")
                print(f"  å¹³å‡æˆåŠŸç‡: {report['æˆåŠŸç‡']['å¹³å‡å€¼']}")
                
                current_stats = report['å½“å‰ç»Ÿè®¡']
                print(f"\nğŸ“ˆ çˆ¬å–ç»Ÿè®¡:")
                print(f"  è¿è¡Œæ—¶é—´: {current_stats['è¿è¡Œæ—¶é—´']}")
                print(f"  æ€»è¯·æ±‚æ•°: {current_stats['æ€»è¯·æ±‚æ•°']}")
                print(f"  æˆåŠŸç‡: {current_stats['æˆåŠŸç‡']}")
                print(f"  å¹³å‡è¯·æ±‚é€Ÿç‡: {current_stats['å¹³å‡è¯·æ±‚é€Ÿç‡']}")
                
                if current_stats['é”™è¯¯ç»Ÿè®¡']:
                    print(f"\nâš ï¸ é”™è¯¯ç»Ÿè®¡:")
                    for error_type, count in current_stats['é”™è¯¯ç»Ÿè®¡'].items():
                        print(f"  {error_type}: {count}æ¬¡")
                        
            # ä¼˜åŒ–å»ºè®®
            suggestions = get_optimization_suggestions()
            if suggestions:
                print(f"\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
                for i, suggestion in enumerate(suggestions, 1):
                    print(f"  {i}. {suggestion}")
                    
            print("\n" + "="*60)
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            
def main():
    parser = argparse.ArgumentParser(description='å¼‚æ­¥è¾¾äººæ•°æ®çˆ¬å–å·¥å…·')
    parser.add_argument('--start', type=int, required=True, help='èµ·å§‹é¡µç ')
    parser.add_argument('--end', type=int, required=True, help='ç»“æŸé¡µç ')
    parser.add_argument('--workers', type=int, default=5, help='çº¿ç¨‹æ•°é‡ (é»˜è®¤: 5)')
    parser.add_argument('--domain-filter', type=str, help='è¾¾äººç±»å‹ç­›é€‰ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”')
    parser.add_argument('--task-name', type=str, help='ä»»åŠ¡åç§°')
    parser.add_argument('--resume-from', type=int, help='ä»æŒ‡å®šé¡µç æ¢å¤çˆ¬å–')
    parser.add_argument('--account-id', type=int, help='æŒ‡å®šä½¿ç”¨çš„è´¦å·ID')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¼‚æ­¥çˆ¬è™«å®ä¾‹
    crawler = AsyncDarenCrawler(max_workers=args.workers)
    
    # å¼€å§‹çˆ¬å–
    crawler.crawl_pages_async(
        start_page=args.start,
        end_page=args.end,
        task_name=args.task_name,
        domain_filter=args.domain_filter,
        resume_from=args.resume_from,
        account_id=args.account_id
    )

if __name__ == "__main__":
    main()