# è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿ

ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒå¤šå¹³å°è¾¾äººæ•°æ®é‡‡é›†ã€æ™ºèƒ½åˆ†ç±»ã€å¤šè´¦å·ç®¡ç†ã€ä»»åŠ¡è°ƒåº¦ç­‰åŠŸèƒ½ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

### æ ¸å¿ƒåŠŸèƒ½
- **æ™ºèƒ½çˆ¬å–**: æ”¯æŒå¤šé¡µé¢å¹¶å‘çˆ¬å–ï¼Œè‡ªåŠ¨å¤„ç†åçˆ¬æœºåˆ¶
- **æ•°æ®åˆ†ç±»**: åŸºäºå†…å®¹æ ‡ç­¾è‡ªåŠ¨åˆ†ç±»è¾¾äººç±»å‹ï¼ˆç¾é£Ÿã€æ—¶å°šã€æ—…æ¸¸ç­‰ï¼‰
- **æ–­ç‚¹ç»­çˆ¬**: æ”¯æŒçˆ¬å–è¿›åº¦ä¿å­˜ï¼Œæ„å¤–ä¸­æ–­åå¯ç»§ç»­çˆ¬å–
- **å¤šè´¦å·ç®¡ç†**: æ™ºèƒ½è´¦å·è½®æ¢ï¼Œé¿å…å•è´¦å·é¢‘ç¹è¯·æ±‚è¢«å°
- **ä»»åŠ¡è°ƒåº¦**: æ”¯æŒå®šæ—¶ä»»åŠ¡ã€ä»»åŠ¡é˜Ÿåˆ—ã€å¤±è´¥é‡è¯•æœºåˆ¶
- **æ•°æ®å…¥åº“**: è‡ªåŠ¨å°†çˆ¬å–æ•°æ®å¯¼å…¥MySQLæ•°æ®åº“
- **ç›‘æ§æŠ¥è­¦**: å®æ—¶ç›‘æ§çˆ¬å–çŠ¶æ€ï¼Œå¼‚å¸¸æƒ…å†µåŠæ—¶æŠ¥è­¦

### æŠ€æœ¯ç‰¹ç‚¹
- **é«˜æ€§èƒ½**: å¤šçº¿ç¨‹å¹¶å‘å¤„ç†ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®çˆ¬å–
- **é«˜å¯é **: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- **æ˜“æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒè‡ªå®šä¹‰æ‰©å±•
- **æ˜“éƒ¨ç½²**: æ”¯æŒDockeréƒ¨ç½²ï¼Œä¸€é”®å¯åŠ¨

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- MySQL 5.7+
- å†…å­˜: å»ºè®®2GB+
- ç£ç›˜: å»ºè®®10GB+å¯ç”¨ç©ºé—´

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿ

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æˆ–ä½¿ç”¨Makefile
make install
```

### 2. é…ç½®æ•°æ®åº“

```bash
# ç”Ÿæˆé…ç½®æ–‡ä»¶
make generate-config

# ç¼–è¾‘.envæ–‡ä»¶ï¼Œé…ç½®æ•°æ®åº“è¿æ¥ä¿¡æ¯
# DB_HOST=localhost
# DB_PORT=3306
# DB_USER=root
# DB_PASSWORD=your_password
# DB_NAME=influencer_crawler

# åˆå§‹åŒ–æ•°æ®åº“
make setup-db
```

### 3. é…ç½®è´¦å·ä¿¡æ¯

```bash
# æ·»åŠ çˆ¬å–è´¦å·
python account_manager.py add --name "è´¦å·1" --cookies "your_cookies_here"

# æŸ¥çœ‹è´¦å·åˆ—è¡¨
python account_manager.py list
```

### 4. å¼€å§‹çˆ¬å–

```bash
# æ–¹å¼1: ç›´æ¥è¿è¡Œçˆ¬è™«
python crawler_main.py --start-page 1 --end-page 10

# æ–¹å¼2: ä½¿ç”¨ä»»åŠ¡è°ƒåº¦å™¨
python task_scheduler.py start

# æ–¹å¼3: ä½¿ç”¨Makefile
make run-crawler
```

## ğŸ“– è¯¦ç»†ä½¿ç”¨æŒ‡å—

### çˆ¬è™«ä½¿ç”¨

#### åŸºæœ¬çˆ¬å–
```bash
# çˆ¬å–æŒ‡å®šé¡µé¢èŒƒå›´
python crawler_main.py --start-page 1 --end-page 100

# æŒ‡å®šè¾¾äººç±»å‹
python crawler_main.py --types ç¾é£Ÿåšä¸» æ—¶å°šåšä¸»

# ä½¿ç”¨æŒ‡å®šè´¦å·
python crawler_main.py --account-id 1

# è®¾ç½®å¹¶å‘æ•°
python crawler_main.py --concurrent 5
```

#### é«˜çº§é€‰é¡¹
```bash
# å¯ç”¨æ–­ç‚¹ç»­çˆ¬
python crawler_main.py --resume

# è®¾ç½®è¯·æ±‚é—´éš”
python crawler_main.py --interval 3

# æŒ‡å®šè¾“å‡ºç›®å½•
python crawler_main.py --output-dir ./data/custom
```

### è´¦å·ç®¡ç†

#### æ·»åŠ è´¦å·
```bash
# åŸºæœ¬æ·»åŠ 
python account_manager.py add --name "æµ‹è¯•è´¦å·" --cookies "cookie_string"

# æ·»åŠ å¸¦è¯·æ±‚å¤´çš„è´¦å·
python account_manager.py add --name "è´¦å·2" --cookies "cookies" --headers '{"User-Agent": "custom"}'

# æ‰¹é‡å¯¼å…¥è´¦å·
python account_manager.py import --file accounts.json
```

#### è´¦å·ç»´æŠ¤
```bash
# æŸ¥çœ‹è´¦å·çŠ¶æ€
python account_manager.py list --status active

# è´¦å·ç»Ÿè®¡ä¿¡æ¯
python account_manager.py stats

# å¥åº·æ£€æŸ¥
python account_manager.py health-check

# æ›´æ–°è´¦å·ä¿¡æ¯
python account_manager.py update --id 1 --status inactive
```

### ä»»åŠ¡è°ƒåº¦

#### åˆ›å»ºä»»åŠ¡
```bash
# åˆ›å»ºçˆ¬å–ä»»åŠ¡
python task_scheduler.py create --type crawl_daoren --name "æ¯æ—¥çˆ¬å–" \
  --params '{"start_page": 1, "end_page": 50}'

# åˆ›å»ºå®šæ—¶ä»»åŠ¡
python task_scheduler.py create --type crawl_daoren --name "å®šæ—¶çˆ¬å–" \
  --schedule "2025-01-24 02:00:00"

# åˆ›å»ºæ•°æ®å¯¼å…¥ä»»åŠ¡
python task_scheduler.py create --type import_data --name "æ•°æ®å¯¼å…¥" \
  --params '{"csv_file": "data/daoren_data.csv"}'
```

#### ä»»åŠ¡ç®¡ç†
```bash
# æŸ¥çœ‹ä»»åŠ¡åˆ—è¡¨
python task_scheduler.py list

# æŸ¥çœ‹ä»»åŠ¡ç»Ÿè®¡
python task_scheduler.py stats

# å–æ¶ˆä»»åŠ¡
python task_scheduler.py cancel --id 123

# é‡è¯•å¤±è´¥ä»»åŠ¡
python task_scheduler.py retry --id 123
```

### æ•°æ®å¯¼å…¥

#### CSVå¯¼å…¥MySQL
```bash
# åŸºæœ¬å¯¼å…¥
python import_to_mysql.py --file data/daoren_data.csv

# æŒ‡å®šæ‰¹æ¬¡å¤§å°
python import_to_mysql.py --file data/daoren_data.csv --batch-size 500

# è·³è¿‡é‡å¤æ•°æ®
python import_to_mysql.py --file data/daoren_data.csv --skip-duplicates
```

## ğŸ—‚ï¸ é¡¹ç›®ç»“æ„

```
è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿ/
â”œâ”€â”€ crawler_main.py          # ä¸»çˆ¬è™«è„šæœ¬
â”œâ”€â”€ import_to_mysql.py       # æ•°æ®å¯¼å…¥è„šæœ¬
â”œâ”€â”€ account_manager.py       # è´¦å·ç®¡ç†è„šæœ¬
â”œâ”€â”€ task_scheduler.py        # ä»»åŠ¡è°ƒåº¦è„šæœ¬
â”œâ”€â”€ init_db.sql             # æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
â”œâ”€â”€ requirements.txt        # Pythonä¾èµ–
â”œâ”€â”€ config.yaml            # é…ç½®æ–‡ä»¶
â”œâ”€â”€ setup.py              # å®‰è£…é…ç½®
â”œâ”€â”€ Makefile             # é¡¹ç›®ç®¡ç†å‘½ä»¤
â”œâ”€â”€ .env                # ç¯å¢ƒå˜é‡é…ç½®
â”œâ”€â”€ README.md          # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ data/             # æ•°æ®å­˜å‚¨ç›®å½•
â”‚   â”œâ”€â”€ ç¾é£Ÿåšä¸»/
â”‚   â”œâ”€â”€ æ—¶å°šåšä¸»/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/            # æ—¥å¿—æ–‡ä»¶ç›®å½•
â”œâ”€â”€ backup/         # æ•°æ®å¤‡ä»½ç›®å½•
â””â”€â”€ scripts/       # è¾…åŠ©è„šæœ¬ç›®å½•
```

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½® (.env)

```bash
# æ•°æ®åº“é…ç½®
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=your_password
DB_NAME=influencer_crawler

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# çˆ¬è™«é…ç½®
REQUEST_TIMEOUT=30
MAX_RETRIES=3
REQUEST_INTERVAL=2

# è´¦å·ç®¡ç†é…ç½®
ACCOUNT_ROTATION_INTERVAL=1800
ACCOUNT_COOLDOWN_PERIOD=300

# ä»»åŠ¡è°ƒåº¦é…ç½®
MAX_WORKER_THREADS=3
```

### é«˜çº§é…ç½® (config.yaml)

è¯¦ç»†é…ç½®é€‰é¡¹è¯·å‚è€ƒ `config.yaml` æ–‡ä»¶ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®åº“è¿æ¥æ± é…ç½®
- çˆ¬è™«è¯·æ±‚å‚æ•°
- è´¦å·è½®æ¢ç­–ç•¥
- ä»»åŠ¡è°ƒåº¦è§„åˆ™
- æ—¥å¿—è¾“å‡ºé…ç½®
- ç›‘æ§æŠ¥è­¦è®¾ç½®

## ğŸ“Š æ•°æ®åº“ç»“æ„

### æ ¸å¿ƒè¡¨ç»“æ„

1. **daoren_types** - è¾¾äººç±»å‹è¡¨
2. **daoren_data** - è¾¾äººæ•°æ®ä¸»è¡¨
3. **daoren_task_log** - ä»»åŠ¡æ—¥å¿—è¡¨
4. **daoren_account** - è´¦å·ç®¡ç†è¡¨
5. **crawl_history** - çˆ¬å–å†å²è®°å½•è¡¨
6. **data_export_log** - æ•°æ®å¯¼å‡ºè®°å½•è¡¨

### æ•°æ®è§†å›¾

1. **v_daoren_stats_by_type** - æŒ‰ç±»å‹ç»Ÿè®¡è¾¾äººæ•°æ®
2. **v_task_execution_stats** - ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡

è¯¦ç»†è¡¨ç»“æ„è¯·å‚è€ƒ `init_db.sql` æ–‡ä»¶ã€‚

## ğŸš€ éƒ¨ç½²æŒ‡å—

### å¼€å‘ç¯å¢ƒéƒ¨ç½²

```bash
# 1. å®‰è£…ä¾èµ–
make install-dev

# 2. é…ç½®ç¯å¢ƒ
make generate-config
# ç¼–è¾‘.envæ–‡ä»¶

# 3. åˆå§‹åŒ–æ•°æ®åº“
make setup-db

# 4. è¿è¡Œæµ‹è¯•
make test

# 5. å¯åŠ¨æœåŠ¡
make run-scheduler
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```bash
# 1. ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# 2. å®‰è£…ç”Ÿäº§ä¾èµ–
pip install -r requirements.txt

# 3. é…ç½®ç”Ÿäº§ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘.envæ–‡ä»¶

# 4. åˆå§‹åŒ–æ•°æ®åº“
make setup-db

# 5. å®‰è£…ç³»ç»ŸæœåŠ¡ (Linux)
make install-service

# 6. å¯åŠ¨æœåŠ¡
sudo systemctl start daoren-scheduler
```

### Dockeréƒ¨ç½²

```bash
# æ„å»ºé•œåƒ
docker build -t daoren-crawler .

# è¿è¡Œå®¹å™¨
docker run -d --name daoren-crawler \
  -v $(pwd)/.env:/app/.env \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/logs:/app/logs \
  daoren-crawler
```

## ğŸ“ˆ ç›‘æ§å’Œç»´æŠ¤

### æ—¥å¸¸ç›‘æ§

```bash
# æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
make task-stats
make list-accounts

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/crawler_main_$(date +%Y%m%d).log
tail -f logs/task_scheduler_$(date +%Y%m%d).log

# å¥åº·æ£€æŸ¥
make health-check
```

### æ•°æ®å¤‡ä»½

```bash
# å¤‡ä»½æ•°æ®åº“
make backup

# å¤‡ä»½é…ç½®æ–‡ä»¶
cp .env .env.backup
cp config.yaml config.yaml.backup
```

### æ€§èƒ½ä¼˜åŒ–

1. **æ•°æ®åº“ä¼˜åŒ–**
   - å®šæœŸæ¸…ç†å†å²æ—¥å¿—
   - ä¼˜åŒ–ç´¢å¼•ç»“æ„
   - è°ƒæ•´è¿æ¥æ± å¤§å°

2. **çˆ¬è™«ä¼˜åŒ–**
   - è°ƒæ•´å¹¶å‘æ•°é‡
   - ä¼˜åŒ–è¯·æ±‚é—´éš”
   - ä½¿ç”¨ä»£ç†æ± 

3. **ç³»ç»Ÿä¼˜åŒ–**
   - ç›‘æ§å†…å­˜ä½¿ç”¨
   - æ¸…ç†ä¸´æ—¶æ–‡ä»¶
   - æ—¥å¿—è½®è½¬

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ•°æ®åº“è¿æ¥å¤±è´¥**
   ```bash
   # æ£€æŸ¥æ•°æ®åº“é…ç½®
   python -c "from dotenv import load_dotenv; load_dotenv(); import os; print(f'DB: {os.getenv(\"DB_HOST\")}:{os.getenv(\"DB_PORT\")}')"
   
   # æµ‹è¯•è¿æ¥
   mysql -h$DB_HOST -P$DB_PORT -u$DB_USER -p$DB_PASSWORD
   ```

2. **è´¦å·è¢«å°ç¦**
   ```bash
   # æŸ¥çœ‹è´¦å·çŠ¶æ€
   python account_manager.py list
   
   # æ›´æ–°è´¦å·çŠ¶æ€
   python account_manager.py update --id 1 --status active
   ```

3. **ä»»åŠ¡æ‰§è¡Œå¤±è´¥**
   ```bash
   # æŸ¥çœ‹ä»»åŠ¡è¯¦æƒ…
   python task_scheduler.py list --status failed
   
   # é‡è¯•å¤±è´¥ä»»åŠ¡
   python task_scheduler.py retry --id 123
   ```

4. **å†…å­˜ä¸è¶³**
   ```bash
   # å‡å°‘å¹¶å‘æ•°
   export MAX_WORKER_THREADS=1
   
   # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
   make clean
   ```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep "ERROR" logs/*.log

# æŸ¥çœ‹çˆ¬å–ç»Ÿè®¡
grep "çˆ¬å–å®Œæˆ" logs/crawler_main_*.log | wc -l

# æŸ¥çœ‹è´¦å·ä½¿ç”¨æƒ…å†µ
grep "è´¦å·è½®æ¢" logs/*.log
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ“ æ”¯æŒ

å¦‚æœæ‚¨åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–å¸®åŠ©ï¼š

1. æŸ¥çœ‹æ–‡æ¡£å’ŒFAQ
2. æäº¤Issue
3. è”ç³»å¼€å‘è€…

## ğŸ”„ æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-01-23)
- âœ¨ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- ğŸš€ æ”¯æŒåŸºæœ¬çˆ¬å–åŠŸèƒ½
- ğŸ“Š æ”¯æŒæ•°æ®åˆ†ç±»å’Œå…¥åº“
- ğŸ‘¥ æ”¯æŒå¤šè´¦å·ç®¡ç†
- â° æ”¯æŒä»»åŠ¡è°ƒåº¦
- ğŸ“ å®Œå–„çš„æ—¥å¿—ç³»ç»Ÿ
- ğŸ› ï¸ ä¸°å¯Œçš„é…ç½®é€‰é¡¹

---

**æ³¨æ„**: è¯·ç¡®ä¿éµå®ˆç›¸å…³ç½‘ç«™çš„robots.txtå’Œä½¿ç”¨æ¡æ¬¾ï¼Œåˆç†ä½¿ç”¨çˆ¬è™«åŠŸèƒ½ã€‚