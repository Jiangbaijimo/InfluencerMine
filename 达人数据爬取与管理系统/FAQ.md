# è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿ - å¸¸è§é—®é¢˜è§£ç­” (FAQ)

æœ¬æ–‡æ¡£æ”¶é›†äº†ç”¨æˆ·åœ¨ä½¿ç”¨è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿæ—¶ç»å¸¸é‡åˆ°çš„é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•

- [å®‰è£…ç›¸å…³é—®é¢˜](#å®‰è£…ç›¸å…³é—®é¢˜)
- [é…ç½®ç›¸å…³é—®é¢˜](#é…ç½®ç›¸å…³é—®é¢˜)
- [çˆ¬å–ç›¸å…³é—®é¢˜](#çˆ¬å–ç›¸å…³é—®é¢˜)
- [æ•°æ®åº“ç›¸å…³é—®é¢˜](#æ•°æ®åº“ç›¸å…³é—®é¢˜)
- [è´¦å·ç®¡ç†é—®é¢˜](#è´¦å·ç®¡ç†é—®é¢˜)
- [æ€§èƒ½ç›¸å…³é—®é¢˜](#æ€§èƒ½ç›¸å…³é—®é¢˜)
- [é”™è¯¯å¤„ç†é—®é¢˜](#é”™è¯¯å¤„ç†é—®é¢˜)
- [éƒ¨ç½²ç›¸å…³é—®é¢˜](#éƒ¨ç½²ç›¸å…³é—®é¢˜)

## ğŸ”§ å®‰è£…ç›¸å…³é—®é¢˜

### Q1: å®‰è£…Pythonä¾èµ–æ—¶å‡ºç°é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: å¸¸è§çš„è§£å†³æ–¹æ¡ˆï¼š

```bash
# 1. å‡çº§pipå’Œsetuptools
pip install --upgrade pip setuptools wheel

# 2. ä½¿ç”¨å›½å†…é•œåƒæº
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/

# 3. å¦‚æœæ˜¯ç‰¹å®šåŒ…å®‰è£…å¤±è´¥ï¼Œå•ç‹¬å®‰è£…
pip install pymysql --no-cache-dir

# 4. åœ¨Windowsä¸Šå¯èƒ½éœ€è¦å®‰è£…Visual C++æ„å»ºå·¥å…·
# ä¸‹è½½å¹¶å®‰è£… Microsoft C++ Build Tools
```

### Q2: MySQLå®‰è£…åæ— æ³•è¿æ¥æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

```bash
# 1. æ£€æŸ¥MySQLæœåŠ¡æ˜¯å¦è¿è¡Œ
# Windows
net start mysql

# Linux
sudo systemctl status mysql

# 2. æ£€æŸ¥ç«¯å£æ˜¯å¦ç›‘å¬
netstat -tlnp | grep 3306

# 3. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
# Windowsé˜²ç«å¢™å…è®¸3306ç«¯å£
# Linux
sudo ufw allow 3306/tcp

# 4. æµ‹è¯•è¿æ¥
mysql -h localhost -u root -p
```

### Q3: è™šæ‹Ÿç¯å¢ƒåˆ›å»ºå¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š

```bash
# 1. ç¡®ä¿Pythonç‰ˆæœ¬æ­£ç¡®
python --version  # åº”è¯¥æ˜¯3.8+

# 2. ä½¿ç”¨å®Œæ•´è·¯å¾„åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# 3. å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨virtualenv
pip install virtualenv
virtualenv venv

# 4. åœ¨Windowsä¸Šå¯èƒ½éœ€è¦æ‰§è¡Œç­–ç•¥
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

## âš™ï¸ é…ç½®ç›¸å…³é—®é¢˜

### Q4: .envæ–‡ä»¶é…ç½®é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥é…ç½®æ ¼å¼ï¼š

```bash
# æ­£ç¡®çš„.envæ ¼å¼ï¼ˆæ³¨æ„æ²¡æœ‰ç©ºæ ¼ï¼‰
DB_HOST=localhost
DB_PORT=3306
DB_USER=crawler_user
DB_PASSWORD=your_password
DB_NAME=influencer_crawler

# é”™è¯¯æ ¼å¼ï¼ˆæœ‰ç©ºæ ¼ï¼‰
DB_HOST = localhost  # é”™è¯¯
DB_PASSWORD="password"  # ä¸éœ€è¦å¼•å·
```

### Q5: å¦‚ä½•éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®ï¼Ÿ

**A**: ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤éªŒè¯ï¼š

```bash
# 1. æ£€æŸ¥ç¯å¢ƒå˜é‡åŠ è½½
python -c "
from dotenv import load_dotenv
import os
load_dotenv()
print('DB_HOST:', os.getenv('DB_HOST'))
print('DB_USER:', os.getenv('DB_USER'))
"

# 2. æµ‹è¯•æ•°æ®åº“è¿æ¥
python -c "
from dotenv import load_dotenv
import pymysql
import os
load_dotenv()
try:
    conn = pymysql.connect(
        host=os.getenv('DB_HOST'),
        port=int(os.getenv('DB_PORT')),
        user=os.getenv('DB_USER'),
        password=os.getenv('DB_PASSWORD'),
        database=os.getenv('DB_NAME')
    )
    print('æ•°æ®åº“è¿æ¥æˆåŠŸ')
    conn.close()
except Exception as e:
    print('è¿æ¥å¤±è´¥:', e)
"
```

### Q6: config.yamlæ–‡ä»¶æ ¼å¼é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: YAMLæ ¼å¼è¦æ±‚ä¸¥æ ¼ï¼š

```yaml
# æ­£ç¡®æ ¼å¼ï¼ˆæ³¨æ„ç¼©è¿›ï¼‰
database:
  host: localhost
  port: 3306
  
crawler:
  interval: 2
  timeout: 30

# é”™è¯¯æ ¼å¼
database:
host: localhost  # ç¼ºå°‘ç¼©è¿›
port:3306        # ç¼ºå°‘ç©ºæ ¼
```

éªŒè¯YAMLæ ¼å¼ï¼š

```bash
python -c "
import yaml
with open('config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)
    print('é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®')
    print(config)
"
```

## ğŸ•·ï¸ çˆ¬å–ç›¸å…³é—®é¢˜

### Q7: çˆ¬å–æ—¶å‡ºç°403/429é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: è¿™é€šå¸¸æ˜¯åçˆ¬è™«æœºåˆ¶è§¦å‘ï¼š

```bash
# 1. å¢åŠ è¯·æ±‚é—´éš”
python crawler_main.py --interval 5 --start-page 1 --end-page 10

# 2. æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ
python account_manager.py health-check

# 3. æ›´æ¢è´¦å·
python account_manager.py add --name "æ–°è´¦å·" --cookies "new_cookie"

# 4. ä½¿ç”¨ä»£ç†ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
python account_manager.py update --name "è´¦å·1" --proxy "http://proxy:8080"
```

### Q8: çˆ¬å–çš„æ•°æ®ä¸å®Œæ•´æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

```bash
# 1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
tail -f logs/crawler.log

# 2. ä½¿ç”¨æµ‹è¯•æ¨¡å¼éªŒè¯
python crawler_main.py --test-mode --verbose

# 3. æ£€æŸ¥ç½‘ç»œè¿æ¥
curl -I "https://www.xiaohongshu.com"

# 4. éªŒè¯Cookieæœ‰æ•ˆæ€§
# åœ¨æµè§ˆå™¨ä¸­è®¿é—®ç›®æ ‡é¡µé¢ï¼Œç¡®è®¤Cookieæœªè¿‡æœŸ
```

### Q9: å¦‚ä½•å¤„ç†çˆ¬å–ä¸­æ–­ï¼Ÿ

**A**: ä½¿ç”¨æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½ï¼š

```bash
# 1. æŸ¥çœ‹ä¸­æ–­çš„ä»»åŠ¡ID
python task_scheduler.py list

# 2. ç»­çˆ¬æŒ‡å®šä»»åŠ¡
python crawler_main.py --resume --task-id 12345

# 3. å¦‚æœæ²¡æœ‰ä»»åŠ¡IDï¼Œä»æŒ‡å®šé¡µé¢å¼€å§‹
python crawler_main.py --start-page 150 --end-page 500
```

### Q10: çˆ¬å–é€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: ä¼˜åŒ–çˆ¬å–æ€§èƒ½ï¼š

```bash
# 1. å‡å°‘è¯·æ±‚é—´éš”ï¼ˆæ³¨æ„ä¸è¦å¤ªæ¿€è¿›ï¼‰
python crawler_main.py --interval 1.5

# 2. å¢åŠ å¹¶å‘æ•°ï¼ˆä¿®æ”¹config.yamlï¼‰
crawler:
  concurrent_requests: 3

# 3. ä½¿ç”¨å¤šä¸ªè´¦å·å¹¶è¡Œ
python account_manager.py add --name "è´¦å·2" --cookies "cookie2"
python account_manager.py add --name "è´¦å·3" --cookies "cookie3"
```

## ğŸ—„ï¸ æ•°æ®åº“ç›¸å…³é—®é¢˜

### Q11: æ•°æ®åº“è¿æ¥æ± è€—å°½æ€ä¹ˆåŠï¼Ÿ

**A**: ä¼˜åŒ–è¿æ¥æ± é…ç½®ï¼š

```yaml
# åœ¨config.yamlä¸­è°ƒæ•´
database:
  pool_size: 20
  pool_recycle: 3600
  pool_timeout: 30
```

æˆ–è€…é‡å¯MySQLæœåŠ¡ï¼š

```bash
# Linux
sudo systemctl restart mysql

# Windows
net stop mysql
net start mysql
```

### Q12: æ•°æ®å¯¼å…¥æ—¶å‡ºç°ç¼–ç é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: ç¡®ä¿ç¼–ç ä¸€è‡´ï¼š

```bash
# 1. æ£€æŸ¥CSVæ–‡ä»¶ç¼–ç 
file -i data/daoren_data.csv

# 2. è½¬æ¢ç¼–ç ï¼ˆå¦‚æœéœ€è¦ï¼‰
iconv -f gbk -t utf-8 data/daoren_data.csv > data/daoren_data_utf8.csv

# 3. æŒ‡å®šç¼–ç å¯¼å…¥
python import_to_mysql.py --file data/daoren_data.csv --encoding utf-8
```

### Q13: æ•°æ®åº“è¡¨æŸåæ€ä¹ˆåŠï¼Ÿ

**A**: ä¿®å¤æ•°æ®åº“è¡¨ï¼š

```sql
-- æ£€æŸ¥è¡¨çŠ¶æ€
CHECK TABLE daoren_data;

-- ä¿®å¤è¡¨
REPAIR TABLE daoren_data;

-- å¦‚æœä¿®å¤å¤±è´¥ï¼Œé‡å»ºè¡¨
CREATE TABLE daoren_data_backup AS SELECT * FROM daoren_data;
DROP TABLE daoren_data;
-- é‡æ–°è¿è¡Œinit_db.sqlåˆ›å»ºè¡¨ç»“æ„
INSERT INTO daoren_data SELECT * FROM daoren_data_backup;
```

### Q14: å¦‚ä½•å¤„ç†é‡å¤æ•°æ®ï¼Ÿ

**A**: æ¸…ç†é‡å¤æ•°æ®ï¼š

```sql
-- æŸ¥æ‰¾é‡å¤æ•°æ®
SELECT daoren_id, COUNT(*) as count 
FROM daoren_data 
GROUP BY daoren_id 
HAVING count > 1;

-- åˆ é™¤é‡å¤æ•°æ®ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
DELETE d1 FROM daoren_data d1
INNER JOIN daoren_data d2 
WHERE d1.id < d2.id AND d1.daoren_id = d2.daoren_id;

-- æ·»åŠ å”¯ä¸€ç´¢å¼•é˜²æ­¢é‡å¤
ALTER TABLE daoren_data ADD UNIQUE INDEX idx_unique_daoren_id (daoren_id);
```

## ğŸ‘¤ è´¦å·ç®¡ç†é—®é¢˜

### Q15: è´¦å·è¢«å°æ€ä¹ˆåŠï¼Ÿ

**A**: å¤„ç†è¢«å°è´¦å·ï¼š

```bash
# 1. ç¦ç”¨è¢«å°è´¦å·
python account_manager.py disable --name "è¢«å°è´¦å·"

# 2. æ·»åŠ æ–°è´¦å·
python account_manager.py add --name "æ–°è´¦å·" --cookies "new_cookie"

# 3. æ£€æŸ¥æ‰€æœ‰è´¦å·çŠ¶æ€
python account_manager.py health-check

# 4. è°ƒæ•´çˆ¬å–ç­–ç•¥
# å¢åŠ è¯·æ±‚é—´éš”ï¼Œå‡å°‘å¹¶å‘æ•°
```

### Q16: å¦‚ä½•æ‰¹é‡ç®¡ç†è´¦å·ï¼Ÿ

**A**: ä½¿ç”¨æ‰¹é‡æ“ä½œï¼š

```bash
# 1. å¯¼å‡ºç°æœ‰è´¦å·
python account_manager.py export --file backup_accounts.json

# 2. ç¼–è¾‘accounts.jsonæ–‡ä»¶æ·»åŠ æ–°è´¦å·
# 3. æ‰¹é‡å¯¼å…¥
python account_manager.py import --file accounts.json

# 4. æ‰¹é‡å¥åº·æ£€æŸ¥
python account_manager.py health-check
```

### Q17: Cookieè¿‡æœŸæ€ä¹ˆåŠï¼Ÿ

**A**: æ›´æ–°Cookieï¼š

```bash
# 1. ä»æµè§ˆå™¨è·å–æ–°Cookie
# æ‰“å¼€å¼€å‘è€…å·¥å…· -> Network -> æ‰¾åˆ°è¯·æ±‚ -> å¤åˆ¶Cookie

# 2. æ›´æ–°è´¦å·Cookie
python account_manager.py update --name "è´¦å·1" --cookies "new_cookie_string"

# 3. éªŒè¯æ›´æ–°
python account_manager.py show --name "è´¦å·1"
```

## ğŸš€ æ€§èƒ½ç›¸å…³é—®é¢˜

### Q18: ç³»ç»Ÿå†…å­˜ä½¿ç”¨è¿‡é«˜æ€ä¹ˆåŠï¼Ÿ

**A**: ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼š

```bash
# 1. å‡å°‘æ‰¹é‡å¤„ç†å¤§å°
python import_to_mysql.py --batch-size 100

# 2. è°ƒæ•´å¹¶å‘æ•°
# ç¼–è¾‘config.yaml
crawler:
  concurrent_requests: 1

# 3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
find data/ -name "*.tmp" -delete

# 4. é‡å¯æœåŠ¡
python task_scheduler.py stop
python task_scheduler.py start
```

### Q19: ç£ç›˜ç©ºé—´ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A**: æ¸…ç†ç£ç›˜ç©ºé—´ï¼š

```bash
# 1. æ¸…ç†æ—¥å¿—æ–‡ä»¶
find logs/ -name "*.log" -mtime +7 -delete

# 2. å‹ç¼©æ—§æ•°æ®æ–‡ä»¶
gzip data/*.csv

# 3. æ¸…ç†æ•°æ®åº“æ—¥å¿—
mysql -u root -p -e "PURGE BINARY LOGS BEFORE DATE_SUB(NOW(), INTERVAL 7 DAY);"

# 4. æ¸…ç†ç³»ç»Ÿä¸´æ—¶æ–‡ä»¶
# Windows
del /q /s %TEMP%\*

# Linux
sudo apt-get clean
```

### Q20: æ•°æ®åº“æŸ¥è¯¢æ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: ä¼˜åŒ–æ•°æ®åº“æ€§èƒ½ï¼š

```sql
-- 1. æ·»åŠ ç´¢å¼•
ALTER TABLE daoren_data ADD INDEX idx_fans_count (fans_count);
ALTER TABLE daoren_data ADD INDEX idx_created_at (created_at);

-- 2. åˆ†æè¡¨
ANALYZE TABLE daoren_data;

-- 3. ä¼˜åŒ–è¡¨
OPTIMIZE TABLE daoren_data;

-- 4. æŸ¥çœ‹æ…¢æŸ¥è¯¢
SHOW VARIABLES LIKE 'slow_query_log';
SET GLOBAL slow_query_log = 'ON';
```

## âŒ é”™è¯¯å¤„ç†é—®é¢˜

### Q21: å¦‚ä½•æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Ÿ

**A**: å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š

```bash
# 1. ä½¿ç”¨verboseæ¨¡å¼
python crawler_main.py --verbose

# 2. æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -f logs/error.log

# 3. æŸ¥çœ‹ç‰¹å®šæ—¶é—´çš„æ—¥å¿—
grep "2024-01-01 12:00" logs/crawler.log

# 4. ç»Ÿè®¡é”™è¯¯ç±»å‹
grep "ERROR" logs/crawler.log | cut -d' ' -f4- | sort | uniq -c
```

### Q22: ç¨‹åºå´©æºƒæ€ä¹ˆåŠï¼Ÿ

**A**: è¯Šæ–­å’Œæ¢å¤ï¼š

```bash
# 1. æŸ¥çœ‹å´©æºƒæ—¥å¿—
tail -n 100 logs/error.log

# 2. æ£€æŸ¥ç³»ç»Ÿèµ„æº
top
df -h

# 3. é‡å¯æœåŠ¡
python task_scheduler.py stop
python task_scheduler.py start

# 4. å¦‚æœæ˜¯æ•°æ®åº“é—®é¢˜
sudo systemctl restart mysql
```

### Q23: ç½‘ç»œè¿æ¥é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼š

```bash
# 1. æµ‹è¯•ç½‘ç»œè¿æ¥
ping www.xiaohongshu.com
curl -I https://www.xiaohongshu.com

# 2. æ£€æŸ¥DNSè§£æ
nslookup www.xiaohongshu.com

# 3. ä½¿ç”¨ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
export http_proxy=http://proxy:8080
export https_proxy=http://proxy:8080

# 4. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
```

## ğŸš€ éƒ¨ç½²ç›¸å…³é—®é¢˜

### Q24: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Ÿ

**A**: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ­¥éª¤ï¼š

```bash
# 1. åˆ›å»ºä¸“ç”¨ç”¨æˆ·
sudo useradd -m -s /bin/bash crawler

# 2. å®‰è£…åˆ°ç³»ç»Ÿç›®å½•
sudo cp -r . /opt/daoren-crawler
sudo chown -R crawler:crawler /opt/daoren-crawler

# 3. åˆ›å»ºç³»ç»ŸæœåŠ¡
sudo cp scripts/daoren-scheduler.service /etc/systemd/system/
sudo systemctl enable daoren-scheduler
sudo systemctl start daoren-scheduler

# 4. é…ç½®æ—¥å¿—è½®è½¬
sudo cp scripts/logrotate.conf /etc/logrotate.d/daoren-crawler
```

### Q25: å¦‚ä½•è®¾ç½®å¼€æœºè‡ªå¯åŠ¨ï¼Ÿ

**A**: é…ç½®è‡ªå¯åŠ¨æœåŠ¡ï¼š

```bash
# 1. åˆ›å»ºæœåŠ¡æ–‡ä»¶
sudo nano /etc/systemd/system/daoren-scheduler.service

# 2. æœåŠ¡æ–‡ä»¶å†…å®¹
[Unit]
Description=Daoren Crawler Scheduler
After=network.target mysql.service

[Service]
Type=simple
User=crawler
WorkingDirectory=/opt/daoren-crawler
ExecStart=/opt/daoren-crawler/venv/bin/python task_scheduler.py start
Restart=always

[Install]
WantedBy=multi-user.target

# 3. å¯ç”¨æœåŠ¡
sudo systemctl enable daoren-scheduler
sudo systemctl start daoren-scheduler
```

### Q26: å¦‚ä½•ç›‘æ§ç³»ç»Ÿè¿è¡ŒçŠ¶æ€ï¼Ÿ

**A**: è®¾ç½®ç›‘æ§ï¼š

```bash
# 1. æŸ¥çœ‹æœåŠ¡çŠ¶æ€
sudo systemctl status daoren-scheduler

# 2. æŸ¥çœ‹å®æ—¶æ—¥å¿—
sudo journalctl -u daoren-scheduler -f

# 3. è®¾ç½®é‚®ä»¶æŠ¥è­¦
# ç¼–è¾‘config.yaml
monitoring:
  alerts:
    enabled: true
    email:
      smtp_server: "smtp.gmail.com"
      recipients: ["admin@example.com"]

# 4. ä½¿ç”¨ç›‘æ§è„šæœ¬
nohup python scripts/monitor.py &
```

## ğŸ”§ å…¶ä»–å¸¸è§é—®é¢˜

### Q27: å¦‚ä½•å¤‡ä»½å’Œæ¢å¤æ•°æ®ï¼Ÿ

**A**: æ•°æ®å¤‡ä»½æ¢å¤ï¼š

```bash
# å¤‡ä»½
mysqldump -u crawler_user -p influencer_crawler > backup.sql
tar -czf data_backup.tar.gz data/

# æ¢å¤
mysql -u crawler_user -p influencer_crawler < backup.sql
tar -xzf data_backup.tar.gz
```

### Q28: å¦‚ä½•å‡çº§ç³»ç»Ÿï¼Ÿ

**A**: ç³»ç»Ÿå‡çº§æ­¥éª¤ï¼š

```bash
# 1. å¤‡ä»½æ•°æ®
make backup

# 2. åœæ­¢æœåŠ¡
python task_scheduler.py stop

# 3. æ›´æ–°ä»£ç 
git pull origin main

# 4. æ›´æ–°ä¾èµ–
pip install -r requirements.txt --upgrade

# 5. è¿è¡Œæ•°æ®åº“è¿ç§»ï¼ˆå¦‚æœæœ‰ï¼‰
python scripts/migrate.py

# 6. é‡å¯æœåŠ¡
python task_scheduler.py start
```

### Q29: å¦‚ä½•è·å–æŠ€æœ¯æ”¯æŒï¼Ÿ

**A**: è·å–å¸®åŠ©çš„æ–¹å¼ï¼š

1. **æŸ¥çœ‹æ–‡æ¡£**ï¼šREADME.md, INSTALL.md, USAGE.md
2. **æ£€æŸ¥æ—¥å¿—**ï¼šlogs/ ç›®å½•ä¸‹çš„æ—¥å¿—æ–‡ä»¶
3. **è¿è¡Œè¯Šæ–­**ï¼š`make check-env`
4. **ç¤¾åŒºæ”¯æŒ**ï¼šGitHub Issues
5. **è”ç³»å¼€å‘è€…**ï¼šé€šè¿‡é‚®ä»¶æˆ–å³æ—¶é€šè®¯å·¥å…·

### Q30: å¦‚ä½•è´¡çŒ®ä»£ç ï¼Ÿ

**A**: å‚ä¸é¡¹ç›®å¼€å‘ï¼š

```bash
# 1. Forké¡¹ç›®
git clone your-fork-url

# 2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
git checkout -b feature/new-feature

# 3. æäº¤æ›´æ”¹
git commit -m "Add new feature"

# 4. æ¨é€åˆ†æ”¯
git push origin feature/new-feature

# 5. åˆ›å»ºPull Request
```

## ğŸ“ è”ç³»æˆ‘ä»¬

å¦‚æœä»¥ä¸ŠFAQæ²¡æœ‰è§£å†³æ‚¨çš„é—®é¢˜ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æˆ‘ä»¬ï¼š

- **GitHub Issues**: æäº¤bugæŠ¥å‘Šæˆ–åŠŸèƒ½è¯·æ±‚
- **é‚®ä»¶æ”¯æŒ**: support@example.com
- **æŠ€æœ¯æ–‡æ¡£**: æŸ¥çœ‹å®Œæ•´çš„æŠ€æœ¯æ–‡æ¡£
- **ç¤¾åŒºè®¨è®º**: åŠ å…¥ç”¨æˆ·äº¤æµç¾¤

---

æœ¬FAQä¼šæŒç»­æ›´æ–°ï¼Œå¦‚æœæ‚¨é‡åˆ°æ–°çš„é—®é¢˜æˆ–æœ‰å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œæ¬¢è¿è´¡çŒ®åˆ°è¿™ä¸ªæ–‡æ¡£ä¸­ã€‚