# è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿ - ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨è¾¾äººæ•°æ®çˆ¬å–ä¸ç®¡ç†ç³»ç»Ÿçš„å„é¡¹åŠŸèƒ½ã€‚

## ğŸ“š ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [åŸºç¡€æ“ä½œ](#åŸºç¡€æ“ä½œ)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [å‘½ä»¤è¡Œå·¥å…·](#å‘½ä»¤è¡Œå·¥å…·)
- [APIæ¥å£](#apiæ¥å£)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ¬¡ä½¿ç”¨

1. **ç¡®è®¤å®‰è£…å®Œæˆ**
   ```bash
   # æ£€æŸ¥ç¯å¢ƒ
   make check-env
   
   # æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
   python task_scheduler.py stats
   ```

2. **æ·»åŠ çˆ¬å–è´¦å·**
   ```bash
   # æ·»åŠ ä¸»è´¦å·
   python account_manager.py add \
     --name "ä¸»è´¦å·" \
     --cookies "your_cookie_here" \
     --notes "ä¸»è¦çˆ¬å–è´¦å·"
   ```

3. **å¼€å§‹ç¬¬ä¸€æ¬¡çˆ¬å–**
   ```bash
   # çˆ¬å–å‰10é¡µæ•°æ®
   python crawler_main.py --start-page 1 --end-page 10
   ```

4. **æŸ¥çœ‹çˆ¬å–ç»“æœ**
   ```bash
   # æŸ¥çœ‹æ•°æ®æ–‡ä»¶
   ls -la data/
   
   # å¯¼å…¥åˆ°æ•°æ®åº“
   python import_to_mysql.py --file data/daoren_data_20240101_120000.csv
   ```

## ğŸ”§ åŸºç¡€æ“ä½œ

### è´¦å·ç®¡ç†

#### æ·»åŠ è´¦å·

```bash
# åŸºæœ¬æ·»åŠ 
python account_manager.py add --name "è´¦å·1" --cookies "cookie_string"

# å®Œæ•´æ·»åŠ 
python account_manager.py add \
  --name "è´¦å·1" \
  --cookies "cookie_string" \
  --headers '{"User-Agent": "Mozilla/5.0..."}' \
  --proxy "http://proxy:8080" \
  --notes "å¤‡ç”¨è´¦å·"
```

#### ç®¡ç†è´¦å·

```bash
# æŸ¥çœ‹æ‰€æœ‰è´¦å·
python account_manager.py list

# æŸ¥çœ‹è´¦å·è¯¦æƒ…
python account_manager.py show --name "è´¦å·1"

# æ›´æ–°è´¦å·
python account_manager.py update --name "è´¦å·1" --cookies "new_cookie"

# ç¦ç”¨è´¦å·
python account_manager.py disable --name "è´¦å·1"

# å¯ç”¨è´¦å·
python account_manager.py enable --name "è´¦å·1"

# åˆ é™¤è´¦å·
python account_manager.py delete --name "è´¦å·1"
```

#### æ‰¹é‡æ“ä½œ

```bash
# æ‰¹é‡å¯¼å…¥è´¦å·
python account_manager.py import --file accounts.json

# æ‰¹é‡å¯¼å‡ºè´¦å·
python account_manager.py export --file backup_accounts.json

# å¥åº·æ£€æŸ¥
python account_manager.py health-check

# æ¸…é™¤å†·å´çŠ¶æ€
python account_manager.py clear-cooldown --name "è´¦å·1"
```

### æ•°æ®çˆ¬å–

#### åŸºæœ¬çˆ¬å–

```bash
# çˆ¬å–æŒ‡å®šé¡µé¢èŒƒå›´
python crawler_main.py --start-page 1 --end-page 100

# ä½¿ç”¨æŒ‡å®šè´¦å·
python crawler_main.py --account "è´¦å·1" --start-page 1 --end-page 50

# æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–å°‘é‡æ•°æ®ï¼‰
python crawler_main.py --test-mode --start-page 1 --end-page 5
```

#### é«˜çº§çˆ¬å–é€‰é¡¹

```bash
# æ–­ç‚¹ç»­çˆ¬
python crawler_main.py --resume --task-id 12345

# æŒ‡å®šè¾“å‡ºç›®å½•
python crawler_main.py --output-dir /path/to/output --start-page 1 --end-page 50

# è®¾ç½®è¯·æ±‚é—´éš”
python crawler_main.py --interval 3 --start-page 1 --end-page 100

# å¯ç”¨è¯¦ç»†æ—¥å¿—
python crawler_main.py --verbose --start-page 1 --end-page 20

# åªçˆ¬å–ç‰¹å®šç±»å‹
python crawler_main.py --types "ç¾é£Ÿ,æ—…æ¸¸" --start-page 1 --end-page 50
```

#### æ‰¹é‡çˆ¬å–é…ç½®

åˆ›å»ºçˆ¬å–é…ç½®æ–‡ä»¶ `crawl_config.json`ï¼š

```json
{
  "tasks": [
    {
      "name": "ç¾é£Ÿè¾¾äººçˆ¬å–",
      "start_page": 1,
      "end_page": 500,
      "types": ["ç¾é£Ÿ", "ç”Ÿæ´»"],
      "account": "è´¦å·1",
      "interval": 2
    },
    {
      "name": "æ—…æ¸¸è¾¾äººçˆ¬å–",
      "start_page": 1,
      "end_page": 300,
      "types": ["æ—…æ¸¸", "æ‘„å½±"],
      "account": "è´¦å·2",
      "interval": 3
    }
  ]
}
```

æ‰§è¡Œæ‰¹é‡çˆ¬å–ï¼š

```bash
python crawler_main.py --config crawl_config.json
```

### æ•°æ®å¯¼å…¥

#### åŸºæœ¬å¯¼å…¥

```bash
# å¯¼å…¥å•ä¸ªæ–‡ä»¶
python import_to_mysql.py --file data/daoren_data_20240101.csv

# æ‰¹é‡å¯¼å…¥ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶
python import_to_mysql.py --directory data/

# å¼ºåˆ¶é‡æ–°å¯¼å…¥ï¼ˆè¦†ç›–å·²å­˜åœ¨æ•°æ®ï¼‰
python import_to_mysql.py --file data.csv --force-reimport
```

#### å¯¼å…¥é€‰é¡¹

```bash
# è·³è¿‡é‡å¤æ•°æ®
python import_to_mysql.py --file data.csv --skip-duplicates

# è®¾ç½®æ‰¹é‡å¤§å°
python import_to_mysql.py --file data.csv --batch-size 500

# å¯ç”¨æ•°æ®éªŒè¯
python import_to_mysql.py --file data.csv --validate

# å¯¼å…¥åæ¸…ç†æºæ–‡ä»¶
python import_to_mysql.py --file data.csv --cleanup
```

### ä»»åŠ¡è°ƒåº¦

#### å¯åŠ¨è°ƒåº¦å™¨

```bash
# å¯åŠ¨ä»»åŠ¡è°ƒåº¦å™¨
python task_scheduler.py start

# åå°è¿è¡Œ
python task_scheduler.py start --daemon

# æŒ‡å®šå·¥ä½œçº¿ç¨‹æ•°
python task_scheduler.py start --workers 5
```

#### ä»»åŠ¡ç®¡ç†

```bash
# åˆ›å»ºå®šæ—¶ä»»åŠ¡
python task_scheduler.py create-schedule \
  --name "æ¯æ—¥çˆ¬å–" \
  --cron "0 2 * * *" \
  --command "python crawler_main.py --start-page 1 --end-page 100"

# æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡
python task_scheduler.py list

# æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
python task_scheduler.py status --task-id 12345

# åœæ­¢ä»»åŠ¡
python task_scheduler.py stop --task-id 12345

# é‡å¯å¤±è´¥ä»»åŠ¡
python task_scheduler.py retry --task-id 12345

# æ¸…ç†å®Œæˆçš„ä»»åŠ¡
python task_scheduler.py cleanup --days 7
```

## ğŸ¯ é«˜çº§åŠŸèƒ½

### æ•°æ®åˆ†æå’Œç»Ÿè®¡

#### åŸºæœ¬ç»Ÿè®¡

```bash
# æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡
python -c "
import pymysql
from dotenv import load_dotenv
import os

load_dotenv()
conn = pymysql.connect(
    host=os.getenv('DB_HOST'),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD'),
    database=os.getenv('DB_NAME')
)

cursor = conn.cursor()
cursor.execute('SELECT COUNT(*) FROM daoren_data')
total = cursor.fetchone()[0]
print(f'æ€»è¾¾äººæ•°é‡: {total}')

cursor.execute('SELECT type_name, COUNT(*) FROM daoren_types GROUP BY type_name')
types = cursor.fetchall()
print('æŒ‰ç±»å‹ç»Ÿè®¡:')
for type_name, count in types:
    print(f'  {type_name}: {count}')

conn.close()
"
```

#### æ•°æ®å¯¼å‡º

```bash
# å¯¼å‡ºæ‰€æœ‰æ•°æ®
python -c "
import pandas as pd
import pymysql
from dotenv import load_dotenv
import os

load_dotenv()
conn = pymysql.connect(
    host=os.getenv('DB_HOST'),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD'),
    database=os.getenv('DB_NAME')
)

# å¯¼å‡ºè¾¾äººæ•°æ®
df = pd.read_sql('SELECT * FROM daoren_data', conn)
df.to_excel('export/all_daoren_data.xlsx', index=False)

# å¯¼å‡ºç»Ÿè®¡æ•°æ®
stats_df = pd.read_sql('SELECT * FROM daoren_stats_by_type', conn)
stats_df.to_excel('export/daoren_statistics.xlsx', index=False)

conn.close()
print('æ•°æ®å¯¼å‡ºå®Œæˆ')
"
```

### ç›‘æ§å’ŒæŠ¥è­¦

#### ç³»ç»Ÿç›‘æ§

```bash
# æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
python task_scheduler.py monitor

# æŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡
python -c "
import psutil
import pymysql
from dotenv import load_dotenv
import os

print('ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ:')
print(f'CPUä½¿ç”¨ç‡: {psutil.cpu_percent()}%')
print(f'å†…å­˜ä½¿ç”¨ç‡: {psutil.virtual_memory().percent}%')
print(f'ç£ç›˜ä½¿ç”¨ç‡: {psutil.disk_usage(\"/\").percent}%')

# æ•°æ®åº“è¿æ¥æ•°
load_dotenv()
conn = pymysql.connect(
    host=os.getenv('DB_HOST'),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD'),
    database=os.getenv('DB_NAME')
)
cursor = conn.cursor()
cursor.execute('SHOW STATUS LIKE \"Threads_connected\"')
connections = cursor.fetchone()[1]
print(f'æ•°æ®åº“è¿æ¥æ•°: {connections}')
conn.close()
"
```

#### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -f logs/error.log

# ç»Ÿè®¡é”™è¯¯ç±»å‹
grep "ERROR" logs/crawler.log | cut -d' ' -f4- | sort | uniq -c | sort -nr

# æŸ¥çœ‹æœ€è¿‘çš„çˆ¬å–è®°å½•
tail -n 100 logs/crawler.log | grep "çˆ¬å–å®Œæˆ"

# åˆ†æè´¦å·ä½¿ç”¨æƒ…å†µ
grep "ä½¿ç”¨è´¦å·" logs/crawler.log | cut -d' ' -f6 | sort | uniq -c
```

### æ•°æ®å¤‡ä»½å’Œæ¢å¤

#### æ•°æ®å¤‡ä»½

```bash
# åˆ›å»ºæ•°æ®åº“å¤‡ä»½
mysqldump -h localhost -u crawler_user -p influencer_crawler > backup/db_backup_$(date +%Y%m%d_%H%M%S).sql

# å¤‡ä»½æ•°æ®æ–‡ä»¶
tar -czf backup/data_backup_$(date +%Y%m%d_%H%M%S).tar.gz data/

# å¤‡ä»½é…ç½®æ–‡ä»¶
cp .env config.yaml backup/
```

#### æ•°æ®æ¢å¤

```bash
# æ¢å¤æ•°æ®åº“
mysql -h localhost -u crawler_user -p influencer_crawler < backup/db_backup_20240101_120000.sql

# æ¢å¤æ•°æ®æ–‡ä»¶
tar -xzf backup/data_backup_20240101_120000.tar.gz
```

### æ€§èƒ½ä¼˜åŒ–

#### æ•°æ®åº“ä¼˜åŒ–

```sql
-- æ·»åŠ ç´¢å¼•
ALTER TABLE daoren_data ADD INDEX idx_daoren_id (daoren_id);
ALTER TABLE daoren_data ADD INDEX idx_fans_count (fans_count);
ALTER TABLE daoren_data ADD INDEX idx_created_at (created_at);

-- åˆ†æè¡¨
ANALYZE TABLE daoren_data;

-- ä¼˜åŒ–è¡¨
OPTIMIZE TABLE daoren_data;
```

#### çˆ¬è™«ä¼˜åŒ–

ç¼–è¾‘ `config.yaml`ï¼š

```yaml
crawler:
  # å¹¶å‘è¯·æ±‚æ•°
  concurrent_requests: 3
  
  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
  request_interval: 1.5
  
  # è¶…æ—¶è®¾ç½®
  timeout: 30
  
  # é‡è¯•æ¬¡æ•°
  max_retries: 3
  
  # è¿æ¥æ± å¤§å°
  connection_pool_size: 10
```

## ğŸ“‹ å‘½ä»¤è¡Œå·¥å…·å‚è€ƒ

### crawler_main.py

```bash
python crawler_main.py [é€‰é¡¹]

é€‰é¡¹:
  --start-page INT        èµ·å§‹é¡µç  (é»˜è®¤: 1)
  --end-page INT         ç»“æŸé¡µç  (é»˜è®¤: 10)
  --account TEXT         æŒ‡å®šä½¿ç”¨çš„è´¦å·åç§°
  --output-dir PATH      è¾“å‡ºç›®å½• (é»˜è®¤: data/)
  --interval FLOAT       è¯·æ±‚é—´éš”ç§’æ•° (é»˜è®¤: 2.0)
  --types TEXT           æŒ‡å®šçˆ¬å–çš„è¾¾äººç±»å‹ï¼Œé€—å·åˆ†éš”
  --test-mode           æµ‹è¯•æ¨¡å¼ï¼Œåªçˆ¬å–å°‘é‡æ•°æ®
  --resume              æ–­ç‚¹ç»­çˆ¬æ¨¡å¼
  --task-id INT         ç»­çˆ¬çš„ä»»åŠ¡ID
  --verbose             å¯ç”¨è¯¦ç»†æ—¥å¿—
  --config PATH         ä½¿ç”¨é…ç½®æ–‡ä»¶
  --help                æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
```

### account_manager.py

```bash
python account_manager.py [å‘½ä»¤] [é€‰é¡¹]

å‘½ä»¤:
  add                   æ·»åŠ æ–°è´¦å·
  list                  åˆ—å‡ºæ‰€æœ‰è´¦å·
  show                  æ˜¾ç¤ºè´¦å·è¯¦æƒ…
  update                æ›´æ–°è´¦å·ä¿¡æ¯
  delete                åˆ é™¤è´¦å·
  enable                å¯ç”¨è´¦å·
  disable               ç¦ç”¨è´¦å·
  stats                 æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
  health-check          å¥åº·æ£€æŸ¥
  clear-cooldown        æ¸…é™¤å†·å´çŠ¶æ€
  import                æ‰¹é‡å¯¼å…¥è´¦å·
  export                æ‰¹é‡å¯¼å‡ºè´¦å·

é€‰é¡¹:
  --name TEXT           è´¦å·åç§°
  --cookies TEXT        Cookieå­—ç¬¦ä¸²
  --headers TEXT        è¯·æ±‚å¤´JSONå­—ç¬¦ä¸²
  --proxy TEXT          ä»£ç†åœ°å€
  --notes TEXT          å¤‡æ³¨ä¿¡æ¯
  --file PATH           æ–‡ä»¶è·¯å¾„
  --help                æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
```

### task_scheduler.py

```bash
python task_scheduler.py [å‘½ä»¤] [é€‰é¡¹]

å‘½ä»¤:
  start                 å¯åŠ¨ä»»åŠ¡è°ƒåº¦å™¨
  stop                  åœæ­¢ä»»åŠ¡è°ƒåº¦å™¨
  status                æŸ¥çœ‹çŠ¶æ€
  list                  åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡
  create-schedule       åˆ›å»ºå®šæ—¶ä»»åŠ¡
  cancel                å–æ¶ˆä»»åŠ¡
  retry                 é‡è¯•å¤±è´¥ä»»åŠ¡
  cleanup               æ¸…ç†å®Œæˆä»»åŠ¡
  monitor               ç›‘æ§ç³»ç»ŸçŠ¶æ€
  stats                 æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯

é€‰é¡¹:
  --daemon              åå°è¿è¡Œ
  --workers INT         å·¥ä½œçº¿ç¨‹æ•°
  --task-id INT         ä»»åŠ¡ID
  --name TEXT           ä»»åŠ¡åç§°
  --cron TEXT           Cronè¡¨è¾¾å¼
  --command TEXT        æ‰§è¡Œå‘½ä»¤
  --days INT            æ¸…ç†å¤©æ•°
  --help                æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
```

### import_to_mysql.py

```bash
python import_to_mysql.py [é€‰é¡¹]

é€‰é¡¹:
  --file PATH           CSVæ–‡ä»¶è·¯å¾„
  --directory PATH      CSVæ–‡ä»¶ç›®å½•
  --batch-size INT      æ‰¹é‡å¤„ç†å¤§å° (é»˜è®¤: 1000)
  --skip-duplicates     è·³è¿‡é‡å¤æ•°æ®
  --force-reimport      å¼ºåˆ¶é‡æ–°å¯¼å…¥
  --validate            å¯ç”¨æ•°æ®éªŒè¯
  --cleanup             å¯¼å…¥åæ¸…ç†æºæ–‡ä»¶
  --help                æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
```

## ğŸ¨ æœ€ä½³å®è·µ

### çˆ¬å–ç­–ç•¥

1. **åˆç†è®¾ç½®è¯·æ±‚é—´éš”**
   ```bash
   # æ¨èé—´éš”2-3ç§’
   python crawler_main.py --interval 2.5 --start-page 1 --end-page 100
   ```

2. **ä½¿ç”¨å¤šè´¦å·è½®æ¢**
   ```bash
   # æ·»åŠ å¤šä¸ªè´¦å·
   python account_manager.py add --name "è´¦å·1" --cookies "cookie1"
   python account_manager.py add --name "è´¦å·2" --cookies "cookie2"
   python account_manager.py add --name "è´¦å·3" --cookies "cookie3"
   
   # ç³»ç»Ÿä¼šè‡ªåŠ¨è½®æ¢ä½¿ç”¨
   ```

3. **åˆ†æ‰¹æ¬¡çˆ¬å–**
   ```bash
   # åˆ†æ—¶æ®µçˆ¬å–ï¼Œé¿å…é›†ä¸­è¯·æ±‚
   # ä¸Šåˆçˆ¬å–
   python crawler_main.py --start-page 1 --end-page 200
   
   # ä¸‹åˆçˆ¬å–
   python crawler_main.py --start-page 201 --end-page 400
   ```

### æ•°æ®ç®¡ç†

1. **å®šæœŸå¤‡ä»½**
   ```bash
   # è®¾ç½®å®šæ—¶å¤‡ä»½ä»»åŠ¡
   python task_scheduler.py create-schedule \
     --name "æ¯æ—¥å¤‡ä»½" \
     --cron "0 3 * * *" \
     --command "make backup"
   ```

2. **æ•°æ®æ¸…ç†**
   ```bash
   # æ¸…ç†7å¤©å‰çš„æ—¥å¿—
   find logs/ -name "*.log" -mtime +7 -delete
   
   # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
   find data/ -name "*.tmp" -delete
   ```

3. **ç›‘æ§æ•°æ®è´¨é‡**
   ```sql
   -- æ£€æŸ¥é‡å¤æ•°æ®
   SELECT daoren_id, COUNT(*) as count 
   FROM daoren_data 
   GROUP BY daoren_id 
   HAVING count > 1;
   
   -- æ£€æŸ¥å¼‚å¸¸æ•°æ®
   SELECT * FROM daoren_data 
   WHERE fans_count < 0 OR fans_count > 100000000;
   ```

### æ€§èƒ½ä¼˜åŒ–

1. **æ•°æ®åº“ä¼˜åŒ–**
   ```sql
   -- å®šæœŸä¼˜åŒ–è¡¨
   OPTIMIZE TABLE daoren_data;
   OPTIMIZE TABLE task_logs;
   
   -- æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
   ANALYZE TABLE daoren_data;
   ```

2. **ç³»ç»Ÿèµ„æºç›‘æ§**
   ```bash
   # ç›‘æ§è„šæœ¬
   #!/bin/bash
   while true; do
     echo "$(date): CPU: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)"
     echo "$(date): Memory: $(free | grep Mem | awk '{printf "%.2f%%", $3/$2 * 100.0}')"
     sleep 300
   done > logs/system_monitor.log &
   ```

### å®‰å…¨å»ºè®®

1. **ä¿æŠ¤æ•æ„Ÿä¿¡æ¯**
   ```bash
   # è®¾ç½®æ–‡ä»¶æƒé™
   chmod 600 .env
   chmod 600 config.yaml
   
   # ä¸è¦åœ¨æ—¥å¿—ä¸­è®°å½•æ•æ„Ÿä¿¡æ¯
   ```

2. **å®šæœŸæ›´æ–°Cookie**
   ```bash
   # å®šæœŸæ£€æŸ¥è´¦å·çŠ¶æ€
   python account_manager.py health-check
   
   # åŠæ—¶æ›´æ–°å¤±æ•ˆçš„Cookie
   python account_manager.py update --name "è´¦å·1" --cookies "new_cookie"
   ```

3. **ç›‘æ§å¼‚å¸¸æ´»åŠ¨**
   ```bash
   # ç›‘æ§é”™è¯¯æ—¥å¿—
   tail -f logs/error.log | grep -i "blocked\|forbidden\|rate limit"
   ```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. çˆ¬å–å¤±è´¥

**é—®é¢˜**: è¯·æ±‚è¢«æ‹’ç»æˆ–è¿”å›é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥è´¦å·çŠ¶æ€
python account_manager.py health-check

# æ›´æ–°Cookie
python account_manager.py update --name "è´¦å·1" --cookies "new_cookie"

# å¢åŠ è¯·æ±‚é—´éš”
python crawler_main.py --interval 5 --start-page 1 --end-page 10
```

#### 2. æ•°æ®åº“è¿æ¥å¤±è´¥

**é—®é¢˜**: æ— æ³•è¿æ¥åˆ°MySQLæ•°æ®åº“

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥MySQLæœåŠ¡
sudo systemctl status mysql

# æµ‹è¯•è¿æ¥
mysql -h localhost -u crawler_user -p

# æ£€æŸ¥é…ç½®æ–‡ä»¶
cat .env | grep DB_
```

#### 3. å†…å­˜ä¸è¶³

**é—®é¢˜**: ç³»ç»Ÿå†…å­˜ä½¿ç”¨è¿‡é«˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°‘å¹¶å‘æ•°
# ç¼–è¾‘config.yaml
crawler:
  concurrent_requests: 1

# å‡å°‘æ‰¹é‡å¤§å°
python import_to_mysql.py --batch-size 100

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
make clean
```

#### 4. ç£ç›˜ç©ºé—´ä¸è¶³

**é—®é¢˜**: ç£ç›˜ç©ºé—´ä¸å¤Ÿ

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ¸…ç†æ—¥å¿—æ–‡ä»¶
find logs/ -name "*.log" -mtime +7 -delete

# å‹ç¼©æ—§æ•°æ®
gzip data/*.csv

# æ¸…ç†æ•°æ®åº“æ—¥å¿—
mysql -u root -p -e "PURGE BINARY LOGS BEFORE DATE_SUB(NOW(), INTERVAL 7 DAY);"
```

### è°ƒè¯•æŠ€å·§

1. **å¯ç”¨è¯¦ç»†æ—¥å¿—**
   ```bash
   python crawler_main.py --verbose --start-page 1 --end-page 1
   ```

2. **ä½¿ç”¨æµ‹è¯•æ¨¡å¼**
   ```bash
   python crawler_main.py --test-mode
   ```

3. **æ£€æŸ¥ç½‘ç»œè¿æ¥**
   ```bash
   curl -I "https://www.xiaohongshu.com"
   ```

4. **åˆ†ææ—¥å¿—æ–‡ä»¶**
   ```bash
   # æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯
   tail -n 50 logs/error.log
   
   # ç»Ÿè®¡é”™è¯¯ç±»å‹
   grep "ERROR" logs/crawler.log | cut -d' ' -f4- | sort | uniq -c
   ```

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ï¼š`logs/` ç›®å½•
2. æ£€æŸ¥é…ç½®æ–‡ä»¶ï¼š`.env` å’Œ `config.yaml`
3. è¿è¡Œç³»ç»Ÿæ£€æŸ¥ï¼š`make check-env`
4. æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€ï¼š`python task_scheduler.py stats`
5. å‚è€ƒæ•…éšœæ’é™¤ç« èŠ‚
6. è”ç³»æŠ€æœ¯æ”¯æŒ

---

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ [README.md](README.md) å’Œ [INSTALL.md](INSTALL.md)ã€‚